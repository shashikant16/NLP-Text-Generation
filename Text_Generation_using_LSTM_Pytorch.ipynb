{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7046a6bdf3bf41d89543a00b93609ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb55187bc45045c3a8859d5798dd32ca",
              "IPY_MODEL_50b03dedd9a64f2cbf269c9ce6492507",
              "IPY_MODEL_a3e8e69174ec4bd6bbdcc26644b597fd"
            ],
            "layout": "IPY_MODEL_ddde06dc86844c018d68705cdc99974d"
          }
        },
        "fb55187bc45045c3a8859d5798dd32ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa09871507e5480db1d7753b13741230",
            "placeholder": "​",
            "style": "IPY_MODEL_9505933019b2498aad4c7fc08933a6f3",
            "value": "100%"
          }
        },
        "50b03dedd9a64f2cbf269c9ce6492507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d44fe7b7ee4a41bf273931b926d43d",
            "max": 64776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6cfdb6143e3548609e020f8f42c45d75",
            "value": 64776
          }
        },
        "a3e8e69174ec4bd6bbdcc26644b597fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9de26a0d8ed4116b6a5d700bdc59b8f",
            "placeholder": "​",
            "style": "IPY_MODEL_9ac1e2a389d84db88c6256de51f4f7d5",
            "value": " 64776/64776 [00:21&lt;00:00, 3237.98it/s]"
          }
        },
        "ddde06dc86844c018d68705cdc99974d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa09871507e5480db1d7753b13741230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9505933019b2498aad4c7fc08933a6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84d44fe7b7ee4a41bf273931b926d43d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cfdb6143e3548609e020f8f42c45d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9de26a0d8ed4116b6a5d700bdc59b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac1e2a389d84db88c6256de51f4f7d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2p_bqZM3WYu"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. Import Libraries\n",
        "2. Load Dataset\n",
        "3. Preprocessing and Exploring Text Data\n",
        "  \n",
        "  3.1 Text Cleaning\n",
        "  \n",
        "  3.2 Finding Word Count\n",
        "\n",
        "  3.3 Find and Replace Rare Words with \"Unknown\" Token\n",
        "\n",
        "4. Data Preparation\n",
        "\n",
        "  4.1 Prepare Sequences\n",
        "\n",
        "  4.2 Create Token-Integer Mappings\n",
        "\n",
        "  4.3 Split Data into Train and Validation Sets\n",
        "\n",
        "  4.4 Pad Sequences\n",
        "\n",
        "  4.5 Convert Text Sequences to Integer Sequences\n",
        "5. Model Building\n",
        "\n",
        "  5.1 Define Model Architecture\n",
        "  \n",
        "  5.2 Start Model Training\n",
        "6. Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZoPOnsX8uPS"
      },
      "source": [
        "# 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_CCxOEI4iZK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import random\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypl18CrLFmt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95fa558-df64-4bcc-a648-53f86dd2487e"
      },
      "source": [
        "# reproducing same results\n",
        "SEED = 2019\n",
        "\n",
        "# torch\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f341cc172f0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_5gPvXxWjru"
      },
      "source": [
        "# 2. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTcAv8MwCUep"
      },
      "source": [
        "# open text file and read in data\n",
        "with open(\"dialogs_dataset\", \"rb\") as f:\n",
        "  dialogs = pickle.load(f)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81_SXWZlE6Zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d135054f-982c-4d76-b251-22d87e4de1f1"
      },
      "source": [
        "# number of text sequences\n",
        "len(dialogs)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64776"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSyAzbttAqP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68910ccf-1385-4f0d-886e-e9268887348b"
      },
      "source": [
        "# print 10 random dialogs\n",
        "random.sample(dialogs, 10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['That sounds decent enough, I want to see it with my friend Adam',\n",
              " 'A iced pumpkin spice chai latte',\n",
              " \"I would like to order a pizza from Bella Vita's\",\n",
              " 'I need to go to Berkeley',\n",
              " \"I want one of the specialty pizzas with all the meat on it - I think it's called the Meatzza\",\n",
              " 'I want to have good sushi',\n",
              " 'I would like the Sooner Tornado pizza please',\n",
              " \"No, I'm all set\",\n",
              " 'Excellent, I appreciate the help',\n",
              " \" That'll let us grab dinner downtown before we go\"]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGGTGRUDW9I3"
      },
      "source": [
        "# 3. Preprocessing and Exploring Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crvUl_ngM8sb"
      },
      "source": [
        "## 3.1 Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUuUKZgUFhkl"
      },
      "source": [
        "# text cleaning\n",
        "dialogs_clean = []\n",
        "\n",
        "for i in dialogs:\n",
        "  # remove everything except alphabets\n",
        "  i = re.sub(\"[^a-zA-Z' ]\", \"\", i)\n",
        "  # convert text to lowercase\n",
        "  i = i.lower()\n",
        "  # add cleaned text to the list\n",
        "  dialogs_clean.append(i)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgZwFK1eSjkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761afd39-df45-4600-c249-7e6b0e64d3c1"
      },
      "source": [
        "random.sample(dialogs_clean, 10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['can you describe it',\n",
              " ' can you order that',\n",
              " \"i'll have the caramel cloud macchiato in grande\",\n",
              " \" i mean every time i start it up the engine runs for a minute or so sputters like it isn't getting enough gas and then dies\",\n",
              " ' is there anything around that time',\n",
              " 'i want a coldbrew',\n",
              " ' it that preassigned seating',\n",
              " 'do you think i have done that its been two weeks since i notice the sound i had to wait until i had a free check to get the work done',\n",
              " \"yes we'll just watch it regular\",\n",
              " 'ok then get me a large razorback supreme deep pan pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYDzFQVvNA7_"
      },
      "source": [
        "\n",
        "## 3.2 Finding Word Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1hrCYRp11UI"
      },
      "source": [
        "# get list of all the words\n",
        "all_words = \" \".join(dialogs_clean).split()\n",
        "\n",
        "words_dict = {}\n",
        "\n",
        "# add word-count pair to the dictionary\n",
        "for word in all_words:\n",
        "  # check if the word is already in dictionary\n",
        "  if word in words_dict:\n",
        "    # increment count of word by 1\n",
        "    words_dict[word] = words_dict[word] + 1\n",
        "  else:\n",
        "    # add the word to dictionary with count 1\n",
        "    words_dict[word] = 1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNxSGPubWaqA"
      },
      "source": [
        "# prepare a dataframe\n",
        "words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())})\n",
        "\n",
        "# sort words by their count in increasing order\n",
        "words_df = words_df.sort_values(by = ['count'])\n",
        "\n",
        "# reset dataframe index\n",
        "words_df.reset_index(inplace = True, drop=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVPbPsSWo-Ak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7afb207-eca8-4e22-bbd5-45ad15284ffb"
      },
      "source": [
        "# vocabulary size\n",
        "len(words_df)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11147"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTwmmOiEXBHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f382ca02-928c-4bc5-fdcf-698403f00387"
      },
      "source": [
        "words_df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         word  count\n",
              "0     tornado      1\n",
              "1         vfw      1\n",
              "2      evelyn      1\n",
              "3  stoplights      1\n",
              "4       vicky      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ed45022-0939-44ef-9ac5-32e75365ae88\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tornado</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vfw</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>evelyn</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stoplights</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vicky</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ed45022-0939-44ef-9ac5-32e75365ae88')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ed45022-0939-44ef-9ac5-32e75365ae88 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ed45022-0939-44ef-9ac5-32e75365ae88');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2a8ce49f-ffe7-4458-8e85-d559cdc79a3f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a8ce49f-ffe7-4458-8e85-d559cdc79a3f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2a8ce49f-ffe7-4458-8e85-d559cdc79a3f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "words_df",
              "summary": "{\n  \"name\": \"words_df\",\n  \"rows\": 11147,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11147,\n        \"samples\": [\n          \"january\",\n          \"gasket\",\n          \"laughs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 424,\n        \"min\": 1,\n        \"max\": 19654,\n        \"num_unique_values\": 430,\n        \"samples\": [\n          7709,\n          76,\n          199\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWJEto8TMPiq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "b02b356d-83cb-4f9f-ec58-8109c4b14c0f"
      },
      "source": [
        "words_df.tail()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      word  count\n",
              "11142  you  11909\n",
              "11143    a  13380\n",
              "11144   to  14000\n",
              "11145  the  15406\n",
              "11146    i  19654"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ba05fe1-a232-418d-b487-2eacf5c946bd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11142</th>\n",
              "      <td>you</td>\n",
              "      <td>11909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11143</th>\n",
              "      <td>a</td>\n",
              "      <td>13380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11144</th>\n",
              "      <td>to</td>\n",
              "      <td>14000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11145</th>\n",
              "      <td>the</td>\n",
              "      <td>15406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11146</th>\n",
              "      <td>i</td>\n",
              "      <td>19654</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ba05fe1-a232-418d-b487-2eacf5c946bd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3ba05fe1-a232-418d-b487-2eacf5c946bd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3ba05fe1-a232-418d-b487-2eacf5c946bd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-04fa8644-abfc-47ab-b7a8-c6cee894b2d7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-04fa8644-abfc-47ab-b7a8-c6cee894b2d7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-04fa8644-abfc-47ab-b7a8-c6cee894b2d7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"words_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"a\",\n          \"i\",\n          \"to\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2954,\n        \"min\": 11909,\n        \"max\": 19654,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          13380,\n          19654,\n          14000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTcWd-pYFRob"
      },
      "source": [
        "## 3.3 Find and Replace Rare Words with \"Unknown\" Token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iC4ztG3XIP3"
      },
      "source": [
        "# user specified threshold value\n",
        "rare_thresh = 4\n",
        "\n",
        "# get percentage of rare words in the vocabulary\n",
        "rare_words_count = len(words_df[words_df['count'] < rare_thresh]['word'])\n",
        "total_words = len(words_df)\n",
        "rare_dist = rare_words_count / total_words\n",
        "\n",
        "# coverage percentage of rare words in the corpus\n",
        "rare_cover = words_df[words_df['count'] < rare_thresh]['count'].sum()/words_df['count'].sum()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "helYHQ4BXNK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e241823b-b19c-4677-ce8c-f671827a5c35"
      },
      "source": [
        "print(f\"Rare words distribution in the vocabulary: {rare_dist*100:.2f}\")\n",
        "print(f\"Rare words coverage in the corpus: {rare_cover*100:.2f}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rare words distribution in the vocabulary: 69.03\n",
            "Rare words coverage in the corpus: 2.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJhbRQllXQJk"
      },
      "source": [
        "# extract rare words in a list\n",
        "rare_words = words_df[words_df['count'] < rare_thresh]['word'].tolist()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rare_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neJha3zjktT3",
        "outputId": "61f3f433-f49b-4a20-d50e-8f8764b58640"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tornado',\n",
              " 'vfw',\n",
              " 'evelyn',\n",
              " 'stoplights',\n",
              " 'vicky',\n",
              " 'vale',\n",
              " \"mineo's\",\n",
              " \"meyer's\",\n",
              " 'tot',\n",
              " 'peppermill',\n",
              " 'norwich',\n",
              " 'bridgeton',\n",
              " 'cesar',\n",
              " 'chavez',\n",
              " 'chandler',\n",
              " 'cupping',\n",
              " 'sise',\n",
              " 'lua',\n",
              " 'heating',\n",
              " 'popmpton',\n",
              " 'tio',\n",
              " 'mmy',\n",
              " 'foind',\n",
              " 'eb',\n",
              " 'parthenon',\n",
              " 'errr',\n",
              " \"farley's\",\n",
              " 'fod',\n",
              " 'moutain',\n",
              " \"cranberry's\",\n",
              " 'tomatos',\n",
              " 'oinons',\n",
              " 'katlin',\n",
              " 'relaxation',\n",
              " 'stories',\n",
              " 'melt',\n",
              " 'frapaachino',\n",
              " 'tooth',\n",
              " 'mandolin',\n",
              " 'restaurans',\n",
              " 'shoppes',\n",
              " 'authenticity',\n",
              " 'answers',\n",
              " 'awell',\n",
              " 'prefect',\n",
              " 'burrata',\n",
              " \"paulie's\",\n",
              " 'damon',\n",
              " 'aroogas',\n",
              " 'tavolta',\n",
              " 'servers',\n",
              " 'inc',\n",
              " 'soo',\n",
              " 'assistanike',\n",
              " 'ekse',\n",
              " 'waits',\n",
              " 'username',\n",
              " 'uppermiddle',\n",
              " 'halfcaf',\n",
              " 'tabs',\n",
              " 'comp',\n",
              " 'carte',\n",
              " 'itching',\n",
              " \"medium's\",\n",
              " 'busaba',\n",
              " 'conditions',\n",
              " 'wobble',\n",
              " 'damaging',\n",
              " 'urgency',\n",
              " 'chelmsford',\n",
              " 'sporkie',\n",
              " 'sporting',\n",
              " 'carne',\n",
              " 'attentions',\n",
              " 'dunno',\n",
              " 'yabbi',\n",
              " 'katsu',\n",
              " 'shiro',\n",
              " 'backseat',\n",
              " 'titanic',\n",
              " 'yacht',\n",
              " 'foxy',\n",
              " 'davenport',\n",
              " 'wheelchairaccessible',\n",
              " 'locallysourced',\n",
              " 'comments',\n",
              " 'estrellita',\n",
              " 'poblana',\n",
              " \"bear's\",\n",
              " 'asada',\n",
              " 'schaumburg',\n",
              " 'gulp',\n",
              " 'golbert',\n",
              " 'lycoming',\n",
              " 'muncy',\n",
              " 'sixty',\n",
              " 'homemade',\n",
              " 'carrot',\n",
              " 'gte',\n",
              " 'suasage',\n",
              " 'gravity',\n",
              " 'malleys',\n",
              " 'invest',\n",
              " 'carbon',\n",
              " 'perlini',\n",
              " 'grated',\n",
              " 'sitted',\n",
              " 'mineral',\n",
              " 'waters',\n",
              " 'carrots',\n",
              " 'squash',\n",
              " 'dip',\n",
              " 'centro',\n",
              " 'moines',\n",
              " 'applesauce',\n",
              " 'brimstone',\n",
              " 'travolta',\n",
              " 'sequoia',\n",
              " 'soluitions',\n",
              " 'blocking',\n",
              " 'accelorate',\n",
              " 'enjoyable',\n",
              " 'tribeca',\n",
              " \"sade's\",\n",
              " 'bumpy',\n",
              " 'sb',\n",
              " 'assortment',\n",
              " 'stratosphere',\n",
              " 'noticeable',\n",
              " 'acommodate',\n",
              " 'taiwanese',\n",
              " 'hartsfieldjackson',\n",
              " 'stuart',\n",
              " 'jeremy',\n",
              " 'footprint',\n",
              " 'plant',\n",
              " 'gastropub',\n",
              " 'aswell',\n",
              " 'msy',\n",
              " 'sonesta',\n",
              " 'dank',\n",
              " 'burma',\n",
              " 'prestigious',\n",
              " 'infrequently',\n",
              " 'tokui',\n",
              " \"crave's\",\n",
              " 'regarding',\n",
              " 'faulty',\n",
              " 'drover',\n",
              " 'grills',\n",
              " 'families',\n",
              " 'ichibon',\n",
              " 'towns',\n",
              " 'diana',\n",
              " 'primos',\n",
              " 'troubadour',\n",
              " 'possum',\n",
              " 'inspecion',\n",
              " 'inspector',\n",
              " 'provider',\n",
              " 'emmons',\n",
              " 'souse',\n",
              " 'ihop',\n",
              " 'irlo',\n",
              " 'bronson',\n",
              " 'daughters',\n",
              " 'affect',\n",
              " 'employees',\n",
              " 'sturdier',\n",
              " 'pl',\n",
              " 'lorenzo',\n",
              " 'olivemushroom',\n",
              " 'problemthat',\n",
              " 'menomonie',\n",
              " 'homes',\n",
              " 'v',\n",
              " 'catalan',\n",
              " 'antipasto',\n",
              " 'rolodex',\n",
              " 'kosmic',\n",
              " 'karma',\n",
              " 'shiitake',\n",
              " 'bowtie',\n",
              " 'jut',\n",
              " 'backside',\n",
              " 'regretted',\n",
              " 'bah',\n",
              " 'micheal',\n",
              " 'jordans',\n",
              " 'madelines',\n",
              " 'secret',\n",
              " 'magpie',\n",
              " 'tails',\n",
              " 'coated',\n",
              " 'bisque',\n",
              " 'instantly',\n",
              " \"sloan's\",\n",
              " 'waltham',\n",
              " 'frontrow',\n",
              " 'olivia',\n",
              " 'luxuries',\n",
              " 'factor',\n",
              " 'onefiveone',\n",
              " 'advertised',\n",
              " 'vanti',\n",
              " 'chocalate',\n",
              " 'freshley',\n",
              " 'truluck',\n",
              " 'vereity',\n",
              " 'latt',\n",
              " 'twofold',\n",
              " 'significant',\n",
              " 'getter',\n",
              " 'obon',\n",
              " 'hearthside',\n",
              " 'cautious',\n",
              " 'soluntions',\n",
              " 'calorie',\n",
              " 'unhelpful',\n",
              " 'wellintentioned',\n",
              " 'cappachino',\n",
              " 'redwood',\n",
              " 'indicator',\n",
              " 'spite',\n",
              " 'lucilles',\n",
              " 'reduce',\n",
              " 'traditionally',\n",
              " 'march',\n",
              " 'stuft',\n",
              " 'delite',\n",
              " 'tracey',\n",
              " 'portal',\n",
              " 'skylines',\n",
              " 'jayz',\n",
              " 'newest',\n",
              " 'controlling',\n",
              " 'lifetime',\n",
              " 'bricks',\n",
              " 'rapscallion',\n",
              " \"dickey's\",\n",
              " 'wha',\n",
              " 'battlestar',\n",
              " 'createyourown',\n",
              " 'hobby',\n",
              " 'dined',\n",
              " \"buddy's\",\n",
              " 'chopped',\n",
              " 'scenic',\n",
              " 'electric',\n",
              " 'preposterous',\n",
              " 'flames',\n",
              " 'stumped',\n",
              " 'breather',\n",
              " 'limoncello',\n",
              " \"neptune's\",\n",
              " 'skinner',\n",
              " 'fiji',\n",
              " 'yakyudori',\n",
              " 'kahala',\n",
              " 'saussage',\n",
              " 'prosciuto',\n",
              " 'asilyn',\n",
              " 'jhons',\n",
              " 'nuevo',\n",
              " 'montclair',\n",
              " 'truffles',\n",
              " 'porcini',\n",
              " 'pecorino',\n",
              " 'hoe',\n",
              " 'kietzki',\n",
              " 'lanie',\n",
              " 'greenpeppers',\n",
              " \"luce's\",\n",
              " 'nostalgia',\n",
              " 'correcting',\n",
              " 'hoverbus',\n",
              " 'convoy',\n",
              " 'mira',\n",
              " \"ah'\",\n",
              " 'pizze',\n",
              " 'feb',\n",
              " 'vs',\n",
              " 'jing',\n",
              " 'chuan',\n",
              " 'broiler',\n",
              " 'anthonys',\n",
              " 'sardo',\n",
              " 'parmigianoreggiano',\n",
              " 'pizzaway',\n",
              " 'alma',\n",
              " 'doggle',\n",
              " 'conformation',\n",
              " 'synthetic',\n",
              " 'government',\n",
              " 'poeple',\n",
              " 'eve',\n",
              " 'jr',\n",
              " 'drivercar',\n",
              " 'uberpremium',\n",
              " 'llc',\n",
              " 'veterans',\n",
              " 'levittown',\n",
              " 'babbling',\n",
              " 'fairyland',\n",
              " 'animal',\n",
              " 'pulaski',\n",
              " 'montrose',\n",
              " 'longing',\n",
              " 'advised',\n",
              " 'commercials',\n",
              " 'philip',\n",
              " 'roth',\n",
              " 'charger',\n",
              " 'midlothian',\n",
              " 'rangers',\n",
              " 'kinship',\n",
              " 'neeed',\n",
              " 'skin',\n",
              " 'behaving',\n",
              " 'contacts',\n",
              " 'topeka',\n",
              " 'yusho',\n",
              " 'totoraku',\n",
              " 'alexia',\n",
              " 'giorgio',\n",
              " 'harrier',\n",
              " 'seater',\n",
              " 'managable',\n",
              " \"bush's\",\n",
              " 'mackenie',\n",
              " 'auntiecathiegmx',\n",
              " 'justini',\n",
              " 'ilet',\n",
              " 'indicated',\n",
              " 'scottsville',\n",
              " 'orde',\n",
              " 'aurelios',\n",
              " 'interact',\n",
              " \"mamacita's\",\n",
              " 'bronze',\n",
              " 'taylorsville',\n",
              " 'tung',\n",
              " 'jajang',\n",
              " 'lastly',\n",
              " 'portobello',\n",
              " 'minimum',\n",
              " 'loreto',\n",
              " 'erase',\n",
              " 'eighty',\n",
              " 'metered',\n",
              " 'argyle',\n",
              " 'monnalisa',\n",
              " 'fowler',\n",
              " 'definitly',\n",
              " \"almalfi's\",\n",
              " 'argh',\n",
              " 'bowls',\n",
              " 'refusing',\n",
              " 'mega',\n",
              " 'shoal',\n",
              " 'monroe',\n",
              " 'dhidfhsdofihhotmule',\n",
              " 'incompetent',\n",
              " 'dig',\n",
              " 'cherie',\n",
              " 'recap',\n",
              " 'blurry',\n",
              " 'herndon',\n",
              " 'meter',\n",
              " 'release',\n",
              " 'thor',\n",
              " 'letche',\n",
              " 'craigsplainin',\n",
              " \"stavros'\",\n",
              " 'dreary',\n",
              " 'nighthawk',\n",
              " 'zing',\n",
              " 'muffies',\n",
              " 'younger',\n",
              " 'intelligient',\n",
              " 'blessing',\n",
              " 'heyi',\n",
              " 'crosstown',\n",
              " 'pti',\n",
              " 'amway',\n",
              " 'roundtable',\n",
              " 'pinkitzel',\n",
              " 'cupcake',\n",
              " 'moderate',\n",
              " 'nothats',\n",
              " 'yesbut',\n",
              " \"ma'mthats\",\n",
              " 'akron',\n",
              " 'aomething',\n",
              " 'charlies',\n",
              " 'louie',\n",
              " 'gulch',\n",
              " 'purses',\n",
              " 'fron',\n",
              " \"'lux'\",\n",
              " \"geno's\",\n",
              " 'unflavored',\n",
              " 'leaked',\n",
              " 'hum',\n",
              " 'lucielles',\n",
              " 'harvard',\n",
              " 'stolen',\n",
              " 'mimi',\n",
              " 'popped',\n",
              " 'iphone',\n",
              " \"o'neil\",\n",
              " 'littleton',\n",
              " 'landston',\n",
              " 'hostetter',\n",
              " 'unlock',\n",
              " 'z',\n",
              " 'swiss',\n",
              " 'famiglia',\n",
              " 'verymuch',\n",
              " \"pepperoni's\",\n",
              " 'acworth',\n",
              " 'finances',\n",
              " 'crappy',\n",
              " 'richness',\n",
              " 'defeat',\n",
              " 'laramie',\n",
              " 'somee',\n",
              " 'googlexa',\n",
              " 'manati',\n",
              " 'doral',\n",
              " 'persimmon',\n",
              " 'mauro',\n",
              " 'rope',\n",
              " 'noice',\n",
              " 'pleasr',\n",
              " 'halfway',\n",
              " 'crane',\n",
              " 'smokes',\n",
              " 'ease',\n",
              " 'poplar',\n",
              " \"yesthat'll\",\n",
              " 'thanksjust',\n",
              " 'lenoir',\n",
              " 'possilbe',\n",
              " 'dollop',\n",
              " 'slamming',\n",
              " 'giovannies',\n",
              " 'turnover',\n",
              " 'whirring',\n",
              " 'madhouse',\n",
              " 'preparing',\n",
              " 'bridal',\n",
              " 'stirred',\n",
              " \"carl's\",\n",
              " 'goverment',\n",
              " 'fuzio',\n",
              " 'cantini',\n",
              " 'rounds',\n",
              " 'pizzaeria',\n",
              " 'lcation',\n",
              " 'kohls',\n",
              " 'bandido',\n",
              " \"yau's\",\n",
              " 'wierdos',\n",
              " 'expedite',\n",
              " 'generous',\n",
              " 'carrollton',\n",
              " 'cinnasquares',\n",
              " 'cheezybread',\n",
              " \"rosebud's\",\n",
              " 'infection',\n",
              " 'appreciation',\n",
              " 'cinnimon',\n",
              " 'grecian',\n",
              " \"joh'ns\",\n",
              " 'breweries',\n",
              " 'xpress',\n",
              " 'madisonhilldale',\n",
              " 'crabs',\n",
              " 'hunger',\n",
              " 'maialina',\n",
              " 'marge',\n",
              " 'princess',\n",
              " 'gasket',\n",
              " 'dealt',\n",
              " 'mitch',\n",
              " 'donahue',\n",
              " 'jodi',\n",
              " 'fayettville',\n",
              " 'senor',\n",
              " 'frogs',\n",
              " 'wrapped',\n",
              " 'drinkers',\n",
              " 'overheard',\n",
              " \"malone's\",\n",
              " 'propose',\n",
              " 'states',\n",
              " 'oaktree',\n",
              " 'paint',\n",
              " 'shold',\n",
              " 'reschudle',\n",
              " 'clinkerdaggers',\n",
              " 'hits',\n",
              " 'diiferent',\n",
              " 'obtain',\n",
              " 'authetic',\n",
              " 'alergy',\n",
              " 'klamata',\n",
              " 'whistle',\n",
              " 'vennila',\n",
              " 'confidently',\n",
              " 'franklyn',\n",
              " 'brining',\n",
              " 'grandson',\n",
              " 'jacobson',\n",
              " 'visting',\n",
              " 'thelinden',\n",
              " 'multiplex',\n",
              " 'chama',\n",
              " 'gaucha',\n",
              " 'halsted',\n",
              " 'pressured',\n",
              " 'hasler',\n",
              " 'personalized',\n",
              " 'conducting',\n",
              " 'stroll',\n",
              " 'cheesystuffed',\n",
              " 'ghet',\n",
              " \"'nono'\",\n",
              " 'relocate',\n",
              " 'millbury',\n",
              " 'fedex',\n",
              " 'slide',\n",
              " 'okask',\n",
              " 'bethesda',\n",
              " 'babareeba',\n",
              " 'counts',\n",
              " \"barrington's\",\n",
              " 'acquaman',\n",
              " 'jamaican',\n",
              " 'garbage',\n",
              " 'environmentally',\n",
              " 'piezons',\n",
              " 'hos',\n",
              " 'cardamon',\n",
              " 'thees',\n",
              " 'oredered',\n",
              " 'ti',\n",
              " 'vespucci',\n",
              " 'ollies',\n",
              " 'wean',\n",
              " \"afternoon's\",\n",
              " 'mints',\n",
              " 'ovens',\n",
              " 'fake',\n",
              " 'fayette',\n",
              " 'veto',\n",
              " 'desmoines',\n",
              " 'romare',\n",
              " 'bearden',\n",
              " 'coach',\n",
              " \"luna's\",\n",
              " 'neapolitan',\n",
              " 'blip',\n",
              " 'bloop',\n",
              " 'lessen',\n",
              " 'covell',\n",
              " 'bff',\n",
              " 'middlesex',\n",
              " 'pineaple',\n",
              " 'adrian',\n",
              " 'cafeterias',\n",
              " 'owuld',\n",
              " 'commend',\n",
              " 'miguel',\n",
              " 'magpies',\n",
              " 'lasur',\n",
              " \"jasper's\",\n",
              " 'tickest',\n",
              " 'finn',\n",
              " 'whataboutburger',\n",
              " 'chaya',\n",
              " 'drago',\n",
              " 'startno',\n",
              " 'kalanianaole',\n",
              " 'kai',\n",
              " 'thrown',\n",
              " 'shave',\n",
              " 'defective',\n",
              " 'airbags',\n",
              " 'samir',\n",
              " 'parkchester',\n",
              " 'sensible',\n",
              " 'schooler',\n",
              " 'mountings',\n",
              " 'weak',\n",
              " 'felt',\n",
              " 'approved',\n",
              " 'teacher',\n",
              " 'machitto',\n",
              " 'mondays',\n",
              " 'cobo',\n",
              " 'historical',\n",
              " 'shaped',\n",
              " 'ounce',\n",
              " 'peporonni',\n",
              " 'ceremony',\n",
              " 'honor',\n",
              " 'richardson',\n",
              " 'exception',\n",
              " 'ignored',\n",
              " 'distracted',\n",
              " 'forwarding',\n",
              " 'girly',\n",
              " 'prouding',\n",
              " 'skylight',\n",
              " 'denton',\n",
              " \"mariano's\",\n",
              " 'godfather',\n",
              " \"lazizza's\",\n",
              " 'prague',\n",
              " \"cicero's\",\n",
              " 'rangeline',\n",
              " 'buffets',\n",
              " 'border',\n",
              " 'azteca',\n",
              " 'nbc',\n",
              " 'ketofriendly',\n",
              " 'drizzled',\n",
              " 'winston',\n",
              " 'hightops',\n",
              " 'teasley',\n",
              " 'laville',\n",
              " 'doritos',\n",
              " 'myles',\n",
              " 'selectxl',\n",
              " 'tango',\n",
              " 'tingly',\n",
              " \"'check\",\n",
              " \"engine'\",\n",
              " 'rims',\n",
              " 'ears',\n",
              " 'surprisse',\n",
              " 'flies',\n",
              " 'bugs',\n",
              " 'nuisance',\n",
              " 'fantatsic',\n",
              " 'aviator',\n",
              " 'kollitz',\n",
              " 'renold',\n",
              " 'period',\n",
              " 'jimmie',\n",
              " 'oilmayo',\n",
              " 'everday',\n",
              " 'tuesdsy',\n",
              " 'omni',\n",
              " \"your're\",\n",
              " 'breadstick',\n",
              " 'chrysler',\n",
              " 'pt',\n",
              " 'cruiser',\n",
              " 'siris',\n",
              " 'applebes',\n",
              " 'bridgewater',\n",
              " 'celtics',\n",
              " 'meatless',\n",
              " 'panel',\n",
              " 'dimming',\n",
              " 'familystyle',\n",
              " 'leeds',\n",
              " 'spoilers',\n",
              " 'chilie',\n",
              " 'danielle',\n",
              " 'arrington',\n",
              " 'fitch',\n",
              " 'shouls',\n",
              " \"matthew's\",\n",
              " 'williard',\n",
              " 'stellhorn',\n",
              " 'fairlane',\n",
              " 'thamk',\n",
              " \"val's\",\n",
              " 'linos',\n",
              " 'melisse',\n",
              " 'syrupy',\n",
              " 'lenox',\n",
              " 'att',\n",
              " 'sukothai',\n",
              " 'td',\n",
              " 'consecutive',\n",
              " 'lighthearted',\n",
              " 'pearls',\n",
              " 'harmony',\n",
              " \"bella's\",\n",
              " 'helpfulness',\n",
              " 'allignment',\n",
              " 'capola',\n",
              " 'daly',\n",
              " 'awkward',\n",
              " 'chirping',\n",
              " 'chocotacos',\n",
              " 'exists',\n",
              " \"tiny's\",\n",
              " 'hesitant',\n",
              " 'sputter',\n",
              " 'repblica',\n",
              " 'pinellas',\n",
              " 'twono',\n",
              " 'buzzfeed',\n",
              " 'cursed',\n",
              " 'dictate',\n",
              " 'geniusyou',\n",
              " 'mah',\n",
              " 'boisterous',\n",
              " 'suffer',\n",
              " 'mabels',\n",
              " 'pallido',\n",
              " 'prefered',\n",
              " 'knocked',\n",
              " 'bind',\n",
              " 'dilevred',\n",
              " 'edmond',\n",
              " 'wank',\n",
              " 'feasible',\n",
              " 'hackamoore',\n",
              " 'pistol',\n",
              " 'pirate',\n",
              " 'makepeace',\n",
              " 'inherited',\n",
              " 'grandfather',\n",
              " 'climate',\n",
              " 'estate',\n",
              " 'autoshops',\n",
              " 'orginal',\n",
              " 'missy',\n",
              " 'buzz',\n",
              " 'pepporni',\n",
              " 'willit',\n",
              " 'tenish',\n",
              " 'bryce',\n",
              " 'marker',\n",
              " 'brad',\n",
              " 'gto',\n",
              " 'puchase',\n",
              " 'lists',\n",
              " 'sixthirty',\n",
              " 'attitude',\n",
              " 'porterhouses',\n",
              " 'tesla',\n",
              " 'assumed',\n",
              " 'reccomend',\n",
              " 'ordeing',\n",
              " 'greenville',\n",
              " 'preffred',\n",
              " 'viewin',\n",
              " 'disappoints',\n",
              " \"gwu's\",\n",
              " 'souce',\n",
              " 'evenining',\n",
              " 'razzis',\n",
              " 'prepayment',\n",
              " 'jacey',\n",
              " 'defiantly',\n",
              " \"portillo's\",\n",
              " 'experiment',\n",
              " 'wakemeup',\n",
              " 'meditterannean',\n",
              " 'sriracha',\n",
              " 'superior',\n",
              " 'redbones',\n",
              " \"people's\",\n",
              " 'republik',\n",
              " 'enviorment',\n",
              " 'fiftey',\n",
              " 'ample',\n",
              " \"jeff's\",\n",
              " 'tomrrow',\n",
              " 'annervisery',\n",
              " 'ninety',\n",
              " 'seventy',\n",
              " \"fandango's\",\n",
              " 'ampitheatre',\n",
              " 'zerozero',\n",
              " 'goleta',\n",
              " 'chops',\n",
              " 'cheektowaga',\n",
              " 'mediumonions',\n",
              " 'kale',\n",
              " 'mcuh',\n",
              " 'spacious',\n",
              " 'q',\n",
              " 'torinos',\n",
              " 'utica',\n",
              " 'mediumsized',\n",
              " 'domo',\n",
              " 'kyota',\n",
              " 'ka',\n",
              " \"lelli's\",\n",
              " 'background',\n",
              " 'pleasing',\n",
              " 'waterpump',\n",
              " 'dropbox',\n",
              " 'parmasan',\n",
              " 'inclduing',\n",
              " 'golfland',\n",
              " 'sunsplash',\n",
              " 'slower',\n",
              " 'dearborn',\n",
              " 'forrester',\n",
              " 'tunnel',\n",
              " 'consent',\n",
              " \"d'angelo's\",\n",
              " 'gun',\n",
              " 'wonky',\n",
              " 'kimmel',\n",
              " 'workout',\n",
              " 'tinley',\n",
              " 'governors',\n",
              " 'richton',\n",
              " 'chicagotinley',\n",
              " 'parkconv',\n",
              " 'ctr',\n",
              " 'piercings',\n",
              " 'eyecontacts',\n",
              " 'spirited',\n",
              " 'gsa',\n",
              " 'enetertainment',\n",
              " \"zodo's\",\n",
              " 'rollercoaster',\n",
              " 'tortiose',\n",
              " 'hostel',\n",
              " 'procedure',\n",
              " 'contains',\n",
              " 'shutdown',\n",
              " 'southland',\n",
              " 'pagliacci',\n",
              " 'reveiws',\n",
              " 'seemingly',\n",
              " 'differs',\n",
              " 'startup',\n",
              " 'nonstop',\n",
              " 'morrow',\n",
              " 'swinging',\n",
              " 'tripes',\n",
              " 'yoou',\n",
              " 'winding',\n",
              " 'incident',\n",
              " 'amiss',\n",
              " 'chives',\n",
              " 'jetro',\n",
              " 'twp',\n",
              " 'rahpsody',\n",
              " 'ridiculously',\n",
              " 'ria',\n",
              " 'yeh',\n",
              " 'fuqua',\n",
              " 'suggesstions',\n",
              " 'ottava',\n",
              " 'nonnas',\n",
              " 'whiting',\n",
              " 'zeigler',\n",
              " 'cinc',\n",
              " 'lease',\n",
              " 'rosie',\n",
              " 'll',\n",
              " 'cheerwine',\n",
              " 'minnetrista',\n",
              " 'phil',\n",
              " 'toppping',\n",
              " 'heroes',\n",
              " 'tinsiltown',\n",
              " 'layton',\n",
              " 'worship',\n",
              " 'makeup',\n",
              " 'vapid',\n",
              " \"rang'e\",\n",
              " 'stalla',\n",
              " 'americana',\n",
              " 'congrats',\n",
              " 'sausalito',\n",
              " 'wasamazing',\n",
              " 'measures',\n",
              " 'bertha',\n",
              " 'lobsters',\n",
              " 'sadistic',\n",
              " 'omfg',\n",
              " 'jobs',\n",
              " 'shower',\n",
              " 'rev',\n",
              " 'briefly',\n",
              " 'pauses',\n",
              " 'congress',\n",
              " 'goth',\n",
              " 'cheek',\n",
              " 'clarendon',\n",
              " 'samamtary',\n",
              " 'vodka',\n",
              " 'customized',\n",
              " 'microwave',\n",
              " 'autopopulate',\n",
              " 'billed',\n",
              " 'realistic',\n",
              " 'brigantine',\n",
              " 'roxanne',\n",
              " \"beau's\",\n",
              " 'tar',\n",
              " 'pacmutual',\n",
              " 'coyote',\n",
              " 'communicating',\n",
              " 'fo',\n",
              " 'tarzan',\n",
              " 'runtime',\n",
              " 'abour',\n",
              " 'seasonings',\n",
              " 'napolis',\n",
              " 'presses',\n",
              " 'kosuke',\n",
              " \"domenico's\",\n",
              " 'cat',\n",
              " 'bo',\n",
              " 'comcast',\n",
              " 'browsing',\n",
              " 'lts',\n",
              " 'clinking',\n",
              " \"schmid'ts\",\n",
              " 'contain',\n",
              " 'availabilities',\n",
              " 'milford',\n",
              " 'tha',\n",
              " 'chavel',\n",
              " 'lincohn',\n",
              " 'cgi',\n",
              " 'fairlakes',\n",
              " 'marrifield',\n",
              " 'kingstown',\n",
              " 'greenway',\n",
              " 'okey',\n",
              " 'pancakes',\n",
              " 'booze',\n",
              " 'kyou',\n",
              " \"splenda's\",\n",
              " 'ehhh',\n",
              " 'albright',\n",
              " 'waterford',\n",
              " 'dolphins',\n",
              " 'martins',\n",
              " 'kyle',\n",
              " 'mattersen',\n",
              " 'solved',\n",
              " 'sincerely',\n",
              " 'delorenzos',\n",
              " \"the'll\",\n",
              " 'mailing',\n",
              " \"bialy's\",\n",
              " 'crossgates',\n",
              " 'tcl',\n",
              " 'independent',\n",
              " \"movie's\",\n",
              " 'award',\n",
              " 'christ',\n",
              " 'flingers',\n",
              " 'bq',\n",
              " 'speilburg',\n",
              " 'rephrase',\n",
              " 'kimton',\n",
              " 'lopez',\n",
              " 'history',\n",
              " 'riverfront',\n",
              " 'pooling',\n",
              " 'peppo',\n",
              " 'delete',\n",
              " 'marsh',\n",
              " 'sanibel',\n",
              " 'hatter',\n",
              " 'tourist',\n",
              " 'rhine',\n",
              " 'gaucho',\n",
              " 'sounda',\n",
              " 'amphora',\n",
              " 'sharedlyft',\n",
              " 'luxurary',\n",
              " 'honesty',\n",
              " 'bala',\n",
              " 'mindy',\n",
              " 'apache',\n",
              " 'airlines',\n",
              " 'hines',\n",
              " 'everybody',\n",
              " 'crying',\n",
              " 'safeco',\n",
              " 'parilla',\n",
              " 'argentina',\n",
              " 'poing',\n",
              " 'formaggia',\n",
              " 'pomona',\n",
              " 'groton',\n",
              " 'fruita',\n",
              " 'bleu',\n",
              " 'informatoin',\n",
              " 'kamonegi',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "178uBVztqTDa"
      },
      "source": [
        "Let's see the technique that we will use to replace the rare words/tokens in the dataset with a special token known as the unknown token (\"\\<unk\\>\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5tSChzyoOJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b30c0c-a7db-41a2-8df3-3f9b9f97db04"
      },
      "source": [
        "## example\n",
        "# specify rare words\n",
        "r_words = [\"day\", \"book\"]\n",
        "\n",
        "# build pattern\n",
        "pattern = \"\"\n",
        "for i in r_words:\n",
        "  pattern+= \"{}|\".format(i)\n",
        "\n",
        "print(pattern)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day|book|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZAMqmwPowZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793907c4-e42f-4ab7-8c02-9c012cae14d7"
      },
      "source": [
        "# removing the last element which is \"|\"\n",
        "pattern = pattern[:-1]\n",
        "print(pattern)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day|book\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV1K1ibbo3XI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3831a0-d635-4e5b-b01e-b8f493b7004c"
      },
      "source": [
        "# replace the rare words with the <unk> token\n",
        "sents = [\"it has been a long day\", \"this book is a must read\"]\n",
        "\n",
        "for d in sents:\n",
        "  text = re.sub(pattern, \" <unk> \", d)\n",
        "  print(text)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it has been a long  <unk> \n",
            "this  <unk>  is a must read\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEn-L1_8YBjl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "7046a6bdf3bf41d89543a00b93609ce2",
            "fb55187bc45045c3a8859d5798dd32ca",
            "50b03dedd9a64f2cbf269c9ce6492507",
            "a3e8e69174ec4bd6bbdcc26644b597fd",
            "ddde06dc86844c018d68705cdc99974d",
            "aa09871507e5480db1d7753b13741230",
            "9505933019b2498aad4c7fc08933a6f3",
            "84d44fe7b7ee4a41bf273931b926d43d",
            "6cfdb6143e3548609e020f8f42c45d75",
            "e9de26a0d8ed4116b6a5d700bdc59b8f",
            "9ac1e2a389d84db88c6256de51f4f7d5"
          ]
        },
        "outputId": "458c4a76-0490-4243-a66b-6958453619a7"
      },
      "source": [
        "# create a text pattern from the rare words, like \"word1 | word2 | word3...\"\n",
        "pattern = \"\"\n",
        "for i in rare_words:\n",
        "  pattern+= \" {} |\".format(i)\n",
        "\n",
        "# removing the last element which is \"|\"\n",
        "pattern = pattern[:-1]\n",
        "\n",
        "# empty list\n",
        "dialogs_clean_v2 = []\n",
        "\n",
        "# replace the rare words with the <unk> token\n",
        "for d in tqdm_notebook(dialogs_clean):\n",
        "  text = re.sub(pattern, \" <unk> \", d)\n",
        "  dialogs_clean_v2.append(text)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-846b16966f4e>:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for d in tqdm_notebook(dialogs_clean):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64776 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7046a6bdf3bf41d89543a00b93609ce2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoQwO5FAZSP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99301e76-7e4c-4127-cde6-5a13973dbafd"
      },
      "source": [
        "dialogs_clean_v2[520:530]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['does it serve traditional chinese dessert',\n",
              " 'how much extra time to reach <unk> ',\n",
              " 'ok lets reserve a table for dinner at hakkasan',\n",
              " 'hello i need to get a car please',\n",
              " 'holiday inn <unk> parkconv <unk> convention center drive <unk> park il',\n",
              " 'bowling alley <unk> highway <unk> park il',\n",
              " 'what types of cars does uber have',\n",
              " \"what's the price difference\",\n",
              " 'ok get me the cheapest please',\n",
              " 'ok then get me the next level']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waTukoy1AbVT"
      },
      "source": [
        "# 4. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SvIJQ0brBWW"
      },
      "source": [
        "## 4.1 Prepare Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5BYGcwBF7hX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "81385f6d-6183-4c04-c26e-9e9d44319fb7"
      },
      "source": [
        "# capture length of all the sequences\n",
        "text_word_count = []\n",
        "for i in dialogs_clean_v2:\n",
        "  text_word_count.append(len(i.split()))\n",
        "\n",
        "# plot the sequence lengths\n",
        "pd.Series(text_word_count).hist(bins = 30,range=(0,30))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw8F6bfwjBeo"
      },
      "source": [
        "# function to create sequences of equal length\n",
        "def create_seq(text, seq_len = 5):\n",
        "\n",
        "  sequences = []\n",
        "\n",
        "  if len(text.split()) > seq_len:\n",
        "    for i in range(seq_len, len(text.split())):\n",
        "      # select sequence of tokens\n",
        "      seq = text.split()[i-seq_len:i+1]\n",
        "      # append sequence to the list\n",
        "      sequences.append(\" \".join(seq))\n",
        "\n",
        "    return sequences\n",
        "\n",
        "  else:\n",
        "\n",
        "    return [text]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0psJCgo9QtH"
      },
      "source": [
        "# create sequences of equal length\n",
        "seqs = [create_seq(i) for i in dialogs_clean_v2]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCrf5t5vCI6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f062a20-f071-4ebf-87f9-7fcd0b44bbba"
      },
      "source": [
        "seqs[:10]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"hi i'm looking to book a\",\n",
              "  \"i'm looking to book a table\",\n",
              "  'looking to book a table for',\n",
              "  'to book a table for korean',\n",
              "  'book a table for korean fod'],\n",
              " ['somewhere in southern nyc maybe the',\n",
              "  'in southern nyc maybe the east',\n",
              "  'southern nyc maybe the east village'],\n",
              " [\"we don't want to sit at\",\n",
              "  \"don't want to sit at the\",\n",
              "  'want to sit at the bar',\n",
              "  'to sit at the bar but',\n",
              "  'sit at the bar but anywhere',\n",
              "  'at the bar but anywhere else',\n",
              "  'the bar but anywhere else is',\n",
              "  'bar but anywhere else is fine'],\n",
              " ['what times are available'],\n",
              " [\"yikes we can't do those times\"],\n",
              " ['let me check'],\n",
              " [\"great let's book that\"],\n",
              " [\"no that's it just book\"],\n",
              " ['hi i would like to see',\n",
              "  'i would like to see if',\n",
              "  'would like to see if the',\n",
              "  'like to see if the movie',\n",
              "  'to see if the movie what',\n",
              "  'see if the movie what men',\n",
              "  'if the movie what men want',\n",
              "  'the movie what men want is',\n",
              "  'movie what men want is playing',\n",
              "  'what men want is playing here'],\n",
              " ['yes for me and a friend',\n",
              "  'for me and a friend so',\n",
              "  'me and a friend so two',\n",
              "  'and a friend so two tickets',\n",
              "  'a friend so two tickets please']]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4O3-S6PA78w"
      },
      "source": [
        "# merge list-of-lists into a single list\n",
        "seqs = sum(seqs, [])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwmBsxuqxPRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01704c8-1491-4795-af4c-88807f6b66b0"
      },
      "source": [
        "seqs[:15]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"hi i'm looking to book a\",\n",
              " \"i'm looking to book a table\",\n",
              " 'looking to book a table for',\n",
              " 'to book a table for korean',\n",
              " 'book a table for korean fod',\n",
              " 'somewhere in southern nyc maybe the',\n",
              " 'in southern nyc maybe the east',\n",
              " 'southern nyc maybe the east village',\n",
              " \"we don't want to sit at\",\n",
              " \"don't want to sit at the\",\n",
              " 'want to sit at the bar',\n",
              " 'to sit at the bar but',\n",
              " 'sit at the bar but anywhere',\n",
              " 'at the bar but anywhere else',\n",
              " 'the bar but anywhere else is']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNW5MIpTDufa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be196565-7339-408f-c311-77af70fa9b77"
      },
      "source": [
        "# count of sequences\n",
        "len(seqs)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205346"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0dppnffaEgG"
      },
      "source": [
        "# create input and target sequences (x and y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for s in seqs:\n",
        "  x.append(\" \".join(s.split()[:-1]))\n",
        "  y.append(\" \".join(s.split()[1:]))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27XKPYgLwuOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76b0136-b830-462b-cd91-01d8a08b8c40"
      },
      "source": [
        "x[0], y[0]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"hi i'm looking to book\", \"i'm looking to book a\")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVge-c6WHUx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834c3ede-3391-4e8f-cf78-a2ff452e638c"
      },
      "source": [
        "x[88543], y[88543]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('to drive to several locations', 'drive to several locations do')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzR5aG9wrZJ5"
      },
      "source": [
        "## 4.2 Create Token-Integer Mappings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSgaezAI60ht"
      },
      "source": [
        "# create integer-to-token mapping\n",
        "int2token = {}\n",
        "cnt = 1\n",
        "\n",
        "for w in set(\" \".join(dialogs_clean_v2).split()):\n",
        "  int2token[cnt] = w\n",
        "  cnt+= 1\n",
        "\n",
        "# create token-to-integer mapping\n",
        "token2int = {t: i for i, t in int2token.items()}"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5TSElOk_OQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3611f4f7-dac7-40d0-d7cb-b403cbe2559c"
      },
      "source": [
        "token2int[\"can\"], int2token[1127]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(339, 'chap')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQOk80iBr51V"
      },
      "source": [
        "## 4.3 Split Data into Train and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn6a1aXvAcRl"
      },
      "source": [
        "# train-validation split\n",
        "# input sequences\n",
        "x_tr = x[:150000]\n",
        "x_val = x[150000:]\n",
        "\n",
        "# target sequences\n",
        "y_tr = y[:150000]\n",
        "y_val = y[150000:]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIfKsjE3sxgu"
      },
      "source": [
        "## 4.4 Pad Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlG-sd79sNjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "c7d05940-01af-438a-efc0-d3f84027a074"
      },
      "source": [
        "# plot sequence length in train set\n",
        "text_word_count = []\n",
        "\n",
        "for i in x_tr:\n",
        "  text_word_count.append(len(i.split()))\n",
        "\n",
        "pd.Series(text_word_count).hist(bins = 70,range=(0,30))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2umo7Mns5NKl"
      },
      "source": [
        "# based on the plot above\n",
        "max_text_len = 5\n",
        "\n",
        "# function to perform padding\n",
        "def pad_sequence(seq, n):\n",
        "\n",
        "  # split input sequence into tokens\n",
        "  seq = seq.split()\n",
        "\n",
        "  # check if no. of tokens in input sequence is less than 'n'\n",
        "  if len(seq) < n:\n",
        "    for i in range(n - len(seq)):\n",
        "      seq.append(\"<pad>\")\n",
        "\n",
        "  return \" \".join(seq)\n",
        "\n",
        "# pad text sequences (train set)\n",
        "x_tr_padded = [pad_sequence(s, max_text_len) for s in x_tr]\n",
        "y_tr_padded = [pad_sequence(s, max_text_len) for s in y_tr]\n",
        "\n",
        "# pad text sequences (validation set)\n",
        "x_val_padded = [pad_sequence(s, max_text_len) for s in x_val]\n",
        "y_val_padded = [pad_sequence(s, max_text_len) for s in y_val]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxwn3NyD-HBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c793df4d-2e35-4405-953c-7eb45c23d9f2"
      },
      "source": [
        "x_tr_padded[:20]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"hi i'm looking to book\",\n",
              " \"i'm looking to book a\",\n",
              " 'looking to book a table',\n",
              " 'to book a table for',\n",
              " 'book a table for korean',\n",
              " 'somewhere in southern nyc maybe',\n",
              " 'in southern nyc maybe the',\n",
              " 'southern nyc maybe the east',\n",
              " \"we don't want to sit\",\n",
              " \"don't want to sit at\",\n",
              " 'want to sit at the',\n",
              " 'to sit at the bar',\n",
              " 'sit at the bar but',\n",
              " 'at the bar but anywhere',\n",
              " 'the bar but anywhere else',\n",
              " 'bar but anywhere else is',\n",
              " 'what times are <pad> <pad>',\n",
              " \"yikes we can't do those\",\n",
              " 'let me <pad> <pad> <pad>',\n",
              " \"great let's book <pad> <pad>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPEGI_Xf-ne_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a41b6c-a6f2-40ce-aeb0-b2e806c870e2"
      },
      "source": [
        "y_tr_padded[:20]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"i'm looking to book a\",\n",
              " 'looking to book a table',\n",
              " 'to book a table for',\n",
              " 'book a table for korean',\n",
              " 'a table for korean fod',\n",
              " 'in southern nyc maybe the',\n",
              " 'southern nyc maybe the east',\n",
              " 'nyc maybe the east village',\n",
              " \"don't want to sit at\",\n",
              " 'want to sit at the',\n",
              " 'to sit at the bar',\n",
              " 'sit at the bar but',\n",
              " 'at the bar but anywhere',\n",
              " 'the bar but anywhere else',\n",
              " 'bar but anywhere else is',\n",
              " 'but anywhere else is fine',\n",
              " 'times are available <pad> <pad>',\n",
              " \"we can't do those times\",\n",
              " 'me check <pad> <pad> <pad>',\n",
              " \"let's book that <pad> <pad>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlUnd9v_-4ad"
      },
      "source": [
        "# update mapping dictionaries\n",
        "int2token[0] = \"<pad>\"\n",
        "token2int[\"<pad>\"] = 0\n",
        "\n",
        "# set vocabulary size\n",
        "vocab_size = len(int2token)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLqEIRavtMpY"
      },
      "source": [
        "## 4.5 Convert Text Sequences to Integer Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVxwznePACXC"
      },
      "source": [
        "# function to create integer sequences\n",
        "def get_integer_seq(seq):\n",
        "  return [token2int[w] for w in seq.split()]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7QVnobNARnr"
      },
      "source": [
        "# convert text sequences to integer sequences\n",
        "x_tr_int = [get_integer_seq(i) for i in x_tr_padded]\n",
        "y_tr_int = [get_integer_seq(i) for i in y_tr_padded]\n",
        "\n",
        "x_val_int = [get_integer_seq(i) for i in x_val_padded]\n",
        "y_val_int = [get_integer_seq(i) for i in y_val_padded]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo8g1L_FwAOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7fc1862-06cb-451d-df70-ed1a127a8b6b"
      },
      "source": [
        "x_tr_int[:10]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5110, 2173, 5191, 726, 2687],\n",
              " [2173, 5191, 726, 2687, 1706],\n",
              " [5191, 726, 2687, 1706, 3547],\n",
              " [726, 2687, 1706, 3547, 4458],\n",
              " [2687, 1706, 3547, 4458, 4169],\n",
              " [2880, 4928, 4542, 3104, 3598],\n",
              " [4928, 4542, 3104, 3598, 3579],\n",
              " [4542, 3104, 3598, 3579, 586],\n",
              " [1074, 3894, 4729, 726, 3358],\n",
              " [3894, 4729, 726, 3358, 3637]]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGa_ORxZwDkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6100fdd5-4a88-459a-b311-615cda2bb787"
      },
      "source": [
        "y_tr_int[:10]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2173, 5191, 726, 2687, 1706],\n",
              " [5191, 726, 2687, 1706, 3547],\n",
              " [726, 2687, 1706, 3547, 4458],\n",
              " [2687, 1706, 3547, 4458, 4169],\n",
              " [1706, 3547, 4458, 4169, 3800],\n",
              " [4928, 4542, 3104, 3598, 3579],\n",
              " [4542, 3104, 3598, 3579, 586],\n",
              " [3104, 3598, 3579, 586, 887],\n",
              " [3894, 4729, 726, 3358, 3637],\n",
              " [4729, 726, 3358, 3637, 3579]]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLntLMs5NwGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65e7e9f-0ae2-4b3c-b3b3-9e0e24c98103"
      },
      "source": [
        "# convert lists into numpy arrays\n",
        "x_tr_int = np.array(x_tr_int)\n",
        "y_tr_int = np.array(y_tr_int)\n",
        "\n",
        "x_val_int = np.array(x_val_int)\n",
        "y_val_int = np.array(y_val_int)\n",
        "\n",
        "x_tr_int.shape, y_tr_int.shape, x_val_int.shape, y_val_int.shape"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150000, 5), (150000, 5), (55346, 5), (55346, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y3bZcFywH33"
      },
      "source": [
        "# 5. Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCh2prnryopJ"
      },
      "source": [
        "## 5.1 Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldcidFnPRsOM"
      },
      "source": [
        "# define model architecture\n",
        "\n",
        "## embedding layer:\n",
        "##    input dim = vocab_size,\n",
        "##    ouput dim = 200  # it will generate the word lenght of maximum 200 lenght\n",
        "\n",
        "## LSTM layer:\n",
        "##    input dim = 200\n",
        "##    hidden units = 256\n",
        "##    layers = 2\n",
        "##    output dim = 256\n",
        "\n",
        "## Dropout Layer\n",
        "##    input dim = 256\n",
        "##    output dim = 256\n",
        "\n",
        "## fully connected layer\n",
        "##    input dim = 256\n",
        "##    ouput dim = vocab_size\n",
        "\n",
        "class WordLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.3, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    self.emb_layer = nn.Embedding(vocab_size, 200)\n",
        "\n",
        "    ## define the LSTM\n",
        "    # input data is of shape (batch size, sequence length, no. of features)...\n",
        "    # ...therefore we need batch_first=True\n",
        "    self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)\n",
        "\n",
        "    ## define a dropout layer\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    ## define the fully-connected layer\n",
        "    self.fc = nn.Linear(n_hidden, vocab_size)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    ''' Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state is `hidden`. '''\n",
        "\n",
        "    ## pass input through embedding layer\n",
        "    embedded = self.emb_layer(x)\n",
        "\n",
        "    ## Get the outputs and the new hidden state from the lstm\n",
        "    lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "    ## pass through a dropout layer\n",
        "    out = self.dropout(lstm_output)\n",
        "\n",
        "    ## reshape the tensor to the shape (batch-size*sequence length, hidden units)\n",
        "    out = out.reshape(-1, self.n_hidden)\n",
        "\n",
        "    ## put \"out\" through the fully-connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # return the final output and the hidden state\n",
        "    return out, hidden\n",
        "\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    ''' Initializes hidden state '''\n",
        "    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "    # initialized to zero, for hidden state and cell state of LSTM\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (torch.cuda.is_available()):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "    return hidden"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQq72w9ADLn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32035a25-10fa-465a-9152-08cd1b609ac5"
      },
      "source": [
        "# define and print the net\n",
        "net = WordLSTM()\n",
        "print(net)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(6502, 200)\n",
            "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=6502, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pRtYvAVFCKB"
      },
      "source": [
        "# function to generate batches\n",
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "  # iterate through the arrays\n",
        "  prv = 0\n",
        "\n",
        "  for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "    # batch of input sequences\n",
        "    x = arr_x[prv:n,:]\n",
        "\n",
        "    # batch of target sequences\n",
        "    y = arr_y[prv:n,:]\n",
        "\n",
        "    prv = n\n",
        "\n",
        "    yield x, y"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpV3vbVeytW1"
      },
      "source": [
        "## 5.2 Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHMBy8HQ2GhY"
      },
      "source": [
        "def train(net, epochs=10, batch_size=32, lr=0.001, print_every=32):\n",
        "\n",
        "  # set initial loss to infinite\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  # optimizer\n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "  # loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  if(torch.cuda.is_available()):\n",
        "    # push model to GPU\n",
        "    net.cuda()\n",
        "\n",
        "  counter = 0\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "\n",
        "    # iterate over batches\n",
        "    for x, y in get_batches(x_tr_int, y_tr_int, batch_size):\n",
        "      counter+= 1\n",
        "\n",
        "      # convert arrays to tensors\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "      if(torch.cuda.is_available()):\n",
        "        # push tensors to GPU\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      # initialize hidden state\n",
        "      h = net.init_hidden(batch_size)\n",
        "\n",
        "      # set accumulated gradients to zero\n",
        "      net.zero_grad()\n",
        "\n",
        "      # get the output from the model\n",
        "      output, h = net(inputs, h)\n",
        "\n",
        "      # calculate the loss and perform backprop\n",
        "      loss = criterion(output, targets.view(-1))\n",
        "      loss.backward()\n",
        "\n",
        "      opt.step()\n",
        "\n",
        "      if counter % print_every == 0:\n",
        "        # Get validation loss\n",
        "\n",
        "        val_losses = []\n",
        "\n",
        "        net.eval()\n",
        "        for x, y in get_batches(x_val_int, y_val_int, batch_size):\n",
        "\n",
        "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "          val_h = net.init_hidden(batch_size)\n",
        "\n",
        "          inputs, targets = x, y\n",
        "          if(torch.cuda.is_available()):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "          output, val_h = net(inputs, val_h)\n",
        "\n",
        "          val_loss = criterion(output, targets.view(-1))\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        #save the best model\n",
        "        if np.mean(val_losses) < best_valid_loss:\n",
        "          best_valid_loss = np.mean(val_losses)\n",
        "          torch.save(net.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "        net.train()\n",
        "\n",
        "\n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "              \"Step: {}...\".format(counter),\n",
        "              \"Loss: {:.4f}...\".format(loss.item()),\n",
        "              \"ppl: {:.4f} \".format(np.exp(np.mean(val_losses))),\n",
        "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbGexKELaTNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac192684-d7b4-4ec8-b37d-32558e4c4e2b"
      },
      "source": [
        "# specify batch size\n",
        "batch_size = 64\n",
        "\n",
        "# train the model\n",
        "train(net, batch_size = batch_size, epochs=10)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10... Step: 32... Loss: 6.3986... ppl: 680.5800  Val Loss: 6.5229\n",
            "Epoch: 1/10... Step: 64... Loss: 5.6130... ppl: 338.6354  Val Loss: 5.8249\n",
            "Epoch: 1/10... Step: 96... Loss: 6.0531... ppl: 270.6250  Val Loss: 5.6007\n",
            "Epoch: 1/10... Step: 128... Loss: 5.2457... ppl: 235.0442  Val Loss: 5.4598\n",
            "Epoch: 1/10... Step: 160... Loss: 6.1883... ppl: 207.2279  Val Loss: 5.3338\n",
            "Epoch: 1/10... Step: 192... Loss: 5.7420... ppl: 184.0427  Val Loss: 5.2152\n",
            "Epoch: 1/10... Step: 224... Loss: 5.1543... ppl: 165.4843  Val Loss: 5.1089\n",
            "Epoch: 1/10... Step: 256... Loss: 4.2254... ppl: 152.0245  Val Loss: 5.0240\n",
            "Epoch: 1/10... Step: 288... Loss: 4.7805... ppl: 139.0649  Val Loss: 4.9349\n",
            "Epoch: 1/10... Step: 320... Loss: 4.9038... ppl: 130.1296  Val Loss: 4.8685\n",
            "Epoch: 1/10... Step: 352... Loss: 4.3324... ppl: 121.2891  Val Loss: 4.7982\n",
            "Epoch: 1/10... Step: 384... Loss: 5.0573... ppl: 113.2335  Val Loss: 4.7295\n",
            "Epoch: 1/10... Step: 416... Loss: 4.9072... ppl: 108.2876  Val Loss: 4.6848\n",
            "Epoch: 1/10... Step: 448... Loss: 4.4971... ppl: 103.3823  Val Loss: 4.6384\n",
            "Epoch: 1/10... Step: 480... Loss: 4.2454... ppl: 99.3311  Val Loss: 4.5985\n",
            "Epoch: 1/10... Step: 512... Loss: 4.4897... ppl: 95.6061  Val Loss: 4.5602\n",
            "Epoch: 1/10... Step: 544... Loss: 4.7397... ppl: 91.9432  Val Loss: 4.5212\n",
            "Epoch: 1/10... Step: 576... Loss: 5.5437... ppl: 89.0532  Val Loss: 4.4892\n",
            "Epoch: 1/10... Step: 608... Loss: 4.4519... ppl: 87.2072  Val Loss: 4.4683\n",
            "Epoch: 1/10... Step: 640... Loss: 4.4093... ppl: 83.9497  Val Loss: 4.4302\n",
            "Epoch: 1/10... Step: 672... Loss: 4.3665... ppl: 81.9506  Val Loss: 4.4061\n",
            "Epoch: 1/10... Step: 704... Loss: 4.2948... ppl: 79.7505  Val Loss: 4.3789\n",
            "Epoch: 1/10... Step: 736... Loss: 4.7889... ppl: 78.3739  Val Loss: 4.3615\n",
            "Epoch: 1/10... Step: 768... Loss: 3.8817... ppl: 75.9149  Val Loss: 4.3296\n",
            "Epoch: 1/10... Step: 800... Loss: 4.1903... ppl: 74.4706  Val Loss: 4.3104\n",
            "Epoch: 1/10... Step: 832... Loss: 4.7128... ppl: 72.9374  Val Loss: 4.2896\n",
            "Epoch: 1/10... Step: 864... Loss: 4.1700... ppl: 71.5425  Val Loss: 4.2703\n",
            "Epoch: 1/10... Step: 896... Loss: 4.2822... ppl: 70.3586  Val Loss: 4.2536\n",
            "Epoch: 1/10... Step: 928... Loss: 4.3345... ppl: 68.8896  Val Loss: 4.2325\n",
            "Epoch: 1/10... Step: 960... Loss: 3.9326... ppl: 68.0918  Val Loss: 4.2209\n",
            "Epoch: 1/10... Step: 992... Loss: 3.8193... ppl: 66.3371  Val Loss: 4.1947\n",
            "Epoch: 1/10... Step: 1024... Loss: 3.6470... ppl: 65.2761  Val Loss: 4.1786\n",
            "Epoch: 1/10... Step: 1056... Loss: 4.2697... ppl: 64.1519  Val Loss: 4.1613\n",
            "Epoch: 1/10... Step: 1088... Loss: 3.3929... ppl: 63.0308  Val Loss: 4.1436\n",
            "Epoch: 1/10... Step: 1120... Loss: 4.1750... ppl: 62.2940  Val Loss: 4.1319\n",
            "Epoch: 1/10... Step: 1152... Loss: 3.2707... ppl: 61.6268  Val Loss: 4.1211\n",
            "Epoch: 1/10... Step: 1184... Loss: 4.0226... ppl: 61.0479  Val Loss: 4.1117\n",
            "Epoch: 1/10... Step: 1216... Loss: 3.8171... ppl: 60.0983  Val Loss: 4.0960\n",
            "Epoch: 1/10... Step: 1248... Loss: 4.1306... ppl: 59.0381  Val Loss: 4.0782\n",
            "Epoch: 1/10... Step: 1280... Loss: 4.2126... ppl: 58.1397  Val Loss: 4.0628\n",
            "Epoch: 1/10... Step: 1312... Loss: 4.8721... ppl: 57.9288  Val Loss: 4.0592\n",
            "Epoch: 1/10... Step: 1344... Loss: 3.8233... ppl: 57.0685  Val Loss: 4.0443\n",
            "Epoch: 1/10... Step: 1376... Loss: 4.2198... ppl: 56.0948  Val Loss: 4.0270\n",
            "Epoch: 1/10... Step: 1408... Loss: 3.5929... ppl: 55.3786  Val Loss: 4.0142\n",
            "Epoch: 1/10... Step: 1440... Loss: 4.0709... ppl: 54.9770  Val Loss: 4.0069\n",
            "Epoch: 1/10... Step: 1472... Loss: 3.4157... ppl: 54.5964  Val Loss: 4.0000\n",
            "Epoch: 1/10... Step: 1504... Loss: 4.2464... ppl: 54.5297  Val Loss: 3.9987\n",
            "Epoch: 1/10... Step: 1536... Loss: 4.8608... ppl: 53.8533  Val Loss: 3.9863\n",
            "Epoch: 1/10... Step: 1568... Loss: 3.9478... ppl: 52.8314  Val Loss: 3.9671\n",
            "Epoch: 1/10... Step: 1600... Loss: 3.7172... ppl: 52.4587  Val Loss: 3.9600\n",
            "Epoch: 1/10... Step: 1632... Loss: 5.1906... ppl: 51.6771  Val Loss: 3.9450\n",
            "Epoch: 1/10... Step: 1664... Loss: 3.9816... ppl: 51.1866  Val Loss: 3.9355\n",
            "Epoch: 1/10... Step: 1696... Loss: 3.6355... ppl: 51.0012  Val Loss: 3.9318\n",
            "Epoch: 1/10... Step: 1728... Loss: 3.6932... ppl: 50.3700  Val Loss: 3.9194\n",
            "Epoch: 1/10... Step: 1760... Loss: 4.5035... ppl: 50.3087  Val Loss: 3.9182\n",
            "Epoch: 1/10... Step: 1792... Loss: 3.4942... ppl: 49.8409  Val Loss: 3.9088\n",
            "Epoch: 1/10... Step: 1824... Loss: 4.3402... ppl: 50.0580  Val Loss: 3.9132\n",
            "Epoch: 1/10... Step: 1856... Loss: 5.7358... ppl: 48.9300  Val Loss: 3.8904\n",
            "Epoch: 1/10... Step: 1888... Loss: 3.5996... ppl: 48.2656  Val Loss: 3.8767\n",
            "Epoch: 1/10... Step: 1920... Loss: 3.5007... ppl: 47.8556  Val Loss: 3.8682\n",
            "Epoch: 1/10... Step: 1952... Loss: 3.0079... ppl: 47.5989  Val Loss: 3.8628\n",
            "Epoch: 1/10... Step: 1984... Loss: 2.9177... ppl: 47.5991  Val Loss: 3.8628\n",
            "Epoch: 1/10... Step: 2016... Loss: 3.4532... ppl: 46.8192  Val Loss: 3.8463\n",
            "Epoch: 1/10... Step: 2048... Loss: 4.1604... ppl: 46.5865  Val Loss: 3.8413\n",
            "Epoch: 1/10... Step: 2080... Loss: 3.8175... ppl: 46.3903  Val Loss: 3.8371\n",
            "Epoch: 1/10... Step: 2112... Loss: 4.6738... ppl: 46.3511  Val Loss: 3.8362\n",
            "Epoch: 1/10... Step: 2144... Loss: 3.4758... ppl: 45.9611  Val Loss: 3.8278\n",
            "Epoch: 1/10... Step: 2176... Loss: 5.0836... ppl: 45.5597  Val Loss: 3.8190\n",
            "Epoch: 1/10... Step: 2208... Loss: 4.1708... ppl: 45.6087  Val Loss: 3.8201\n",
            "Epoch: 1/10... Step: 2240... Loss: 3.6195... ppl: 45.4438  Val Loss: 3.8165\n",
            "Epoch: 1/10... Step: 2272... Loss: 3.9524... ppl: 44.7467  Val Loss: 3.8010\n",
            "Epoch: 1/10... Step: 2304... Loss: 3.2488... ppl: 44.3141  Val Loss: 3.7913\n",
            "Epoch: 1/10... Step: 2336... Loss: 4.3558... ppl: 44.1297  Val Loss: 3.7871\n",
            "Epoch: 2/10... Step: 2368... Loss: 3.2238... ppl: 44.4029  Val Loss: 3.7933\n",
            "Epoch: 2/10... Step: 2400... Loss: 3.4175... ppl: 44.1818  Val Loss: 3.7883\n",
            "Epoch: 2/10... Step: 2432... Loss: 3.8991... ppl: 44.3474  Val Loss: 3.7921\n",
            "Epoch: 2/10... Step: 2464... Loss: 4.2524... ppl: 44.0907  Val Loss: 3.7862\n",
            "Epoch: 2/10... Step: 2496... Loss: 4.0784... ppl: 43.5767  Val Loss: 3.7745\n",
            "Epoch: 2/10... Step: 2528... Loss: 3.6343... ppl: 43.3601  Val Loss: 3.7695\n",
            "Epoch: 2/10... Step: 2560... Loss: 5.2692... ppl: 43.1155  Val Loss: 3.7639\n",
            "Epoch: 2/10... Step: 2592... Loss: 4.0737... ppl: 43.1207  Val Loss: 3.7640\n",
            "Epoch: 2/10... Step: 2624... Loss: 3.1921... ppl: 42.9215  Val Loss: 3.7594\n",
            "Epoch: 2/10... Step: 2656... Loss: 3.8260... ppl: 42.7429  Val Loss: 3.7552\n",
            "Epoch: 2/10... Step: 2688... Loss: 3.3135... ppl: 42.3885  Val Loss: 3.7469\n",
            "Epoch: 2/10... Step: 2720... Loss: 3.7824... ppl: 42.0563  Val Loss: 3.7390\n",
            "Epoch: 2/10... Step: 2752... Loss: 3.6054... ppl: 42.2137  Val Loss: 3.7427\n",
            "Epoch: 2/10... Step: 2784... Loss: 4.2202... ppl: 42.0088  Val Loss: 3.7379\n",
            "Epoch: 2/10... Step: 2816... Loss: 4.0439... ppl: 42.0814  Val Loss: 3.7396\n",
            "Epoch: 2/10... Step: 2848... Loss: 3.6370... ppl: 41.5750  Val Loss: 3.7275\n",
            "Epoch: 2/10... Step: 2880... Loss: 4.2034... ppl: 41.6018  Val Loss: 3.7281\n",
            "Epoch: 2/10... Step: 2912... Loss: 3.1663... ppl: 41.2925  Val Loss: 3.7207\n",
            "Epoch: 2/10... Step: 2944... Loss: 3.3015... ppl: 41.3819  Val Loss: 3.7228\n",
            "Epoch: 2/10... Step: 2976... Loss: 3.3132... ppl: 41.0924  Val Loss: 3.7158\n",
            "Epoch: 2/10... Step: 3008... Loss: 4.1274... ppl: 41.0061  Val Loss: 3.7137\n",
            "Epoch: 2/10... Step: 3040... Loss: 3.4618... ppl: 40.8708  Val Loss: 3.7104\n",
            "Epoch: 2/10... Step: 3072... Loss: 3.6741... ppl: 40.6885  Val Loss: 3.7059\n",
            "Epoch: 2/10... Step: 3104... Loss: 3.7993... ppl: 40.6327  Val Loss: 3.7046\n",
            "Epoch: 2/10... Step: 3136... Loss: 4.1790... ppl: 40.4025  Val Loss: 3.6989\n",
            "Epoch: 2/10... Step: 3168... Loss: 3.3826... ppl: 40.4416  Val Loss: 3.6999\n",
            "Epoch: 2/10... Step: 3200... Loss: 3.7780... ppl: 40.5794  Val Loss: 3.7033\n",
            "Epoch: 2/10... Step: 3232... Loss: 4.2034... ppl: 40.1432  Val Loss: 3.6925\n",
            "Epoch: 2/10... Step: 3264... Loss: 2.8532... ppl: 39.9086  Val Loss: 3.6866\n",
            "Epoch: 2/10... Step: 3296... Loss: 4.2192... ppl: 40.0283  Val Loss: 3.6896\n",
            "Epoch: 2/10... Step: 3328... Loss: 4.0194... ppl: 39.7867  Val Loss: 3.6835\n",
            "Epoch: 2/10... Step: 3360... Loss: 3.6770... ppl: 39.4072  Val Loss: 3.6739\n",
            "Epoch: 2/10... Step: 3392... Loss: 3.2070... ppl: 39.7461  Val Loss: 3.6825\n",
            "Epoch: 2/10... Step: 3424... Loss: 4.2920... ppl: 39.3150  Val Loss: 3.6716\n",
            "Epoch: 2/10... Step: 3456... Loss: 3.4907... ppl: 39.2008  Val Loss: 3.6687\n",
            "Epoch: 2/10... Step: 3488... Loss: 3.6199... ppl: 39.3371  Val Loss: 3.6722\n",
            "Epoch: 2/10... Step: 3520... Loss: 3.4255... ppl: 38.9458  Val Loss: 3.6622\n",
            "Epoch: 2/10... Step: 3552... Loss: 4.0178... ppl: 39.0431  Val Loss: 3.6647\n",
            "Epoch: 2/10... Step: 3584... Loss: 2.9567... ppl: 38.8052  Val Loss: 3.6586\n",
            "Epoch: 2/10... Step: 3616... Loss: 3.5332... ppl: 38.6195  Val Loss: 3.6538\n",
            "Epoch: 2/10... Step: 3648... Loss: 3.2048... ppl: 38.7038  Val Loss: 3.6559\n",
            "Epoch: 2/10... Step: 3680... Loss: 3.6978... ppl: 38.5347  Val Loss: 3.6516\n",
            "Epoch: 2/10... Step: 3712... Loss: 3.4978... ppl: 38.3945  Val Loss: 3.6479\n",
            "Epoch: 2/10... Step: 3744... Loss: 3.4267... ppl: 38.1586  Val Loss: 3.6418\n",
            "Epoch: 2/10... Step: 3776... Loss: 3.9637... ppl: 38.3706  Val Loss: 3.6473\n",
            "Epoch: 2/10... Step: 3808... Loss: 5.0594... ppl: 38.0713  Val Loss: 3.6395\n",
            "Epoch: 2/10... Step: 3840... Loss: 4.3886... ppl: 38.2944  Val Loss: 3.6453\n",
            "Epoch: 2/10... Step: 3872... Loss: 3.9725... ppl: 38.0132  Val Loss: 3.6379\n",
            "Epoch: 2/10... Step: 3904... Loss: 2.9638... ppl: 37.8520  Val Loss: 3.6337\n",
            "Epoch: 2/10... Step: 3936... Loss: 3.8710... ppl: 37.9525  Val Loss: 3.6363\n",
            "Epoch: 2/10... Step: 3968... Loss: 3.8978... ppl: 37.5454  Val Loss: 3.6256\n",
            "Epoch: 2/10... Step: 4000... Loss: 3.7449... ppl: 37.5331  Val Loss: 3.6252\n",
            "Epoch: 2/10... Step: 4032... Loss: 4.2036... ppl: 37.4016  Val Loss: 3.6217\n",
            "Epoch: 2/10... Step: 4064... Loss: 3.7044... ppl: 37.3649  Val Loss: 3.6207\n",
            "Epoch: 2/10... Step: 4096... Loss: 3.5131... ppl: 37.4222  Val Loss: 3.6223\n",
            "Epoch: 2/10... Step: 4128... Loss: 2.8598... ppl: 37.3084  Val Loss: 3.6192\n",
            "Epoch: 2/10... Step: 4160... Loss: 2.9033... ppl: 37.4103  Val Loss: 3.6219\n",
            "Epoch: 2/10... Step: 4192... Loss: 3.3646... ppl: 37.0746  Val Loss: 3.6129\n",
            "Epoch: 2/10... Step: 4224... Loss: 3.3128... ppl: 36.7880  Val Loss: 3.6052\n",
            "Epoch: 2/10... Step: 4256... Loss: 3.4179... ppl: 36.9075  Val Loss: 3.6084\n",
            "Epoch: 2/10... Step: 4288... Loss: 3.4106... ppl: 36.7653  Val Loss: 3.6046\n",
            "Epoch: 2/10... Step: 4320... Loss: 2.9254... ppl: 36.7927  Val Loss: 3.6053\n",
            "Epoch: 2/10... Step: 4352... Loss: 3.0221... ppl: 36.6854  Val Loss: 3.6024\n",
            "Epoch: 2/10... Step: 4384... Loss: 3.1631... ppl: 36.5705  Val Loss: 3.5992\n",
            "Epoch: 2/10... Step: 4416... Loss: 3.8625... ppl: 36.6373  Val Loss: 3.6011\n",
            "Epoch: 2/10... Step: 4448... Loss: 3.0186... ppl: 36.7632  Val Loss: 3.6045\n",
            "Epoch: 2/10... Step: 4480... Loss: 4.4124... ppl: 36.6267  Val Loss: 3.6008\n",
            "Epoch: 2/10... Step: 4512... Loss: 3.3263... ppl: 36.3236  Val Loss: 3.5925\n",
            "Epoch: 2/10... Step: 4544... Loss: 3.4378... ppl: 36.3170  Val Loss: 3.5923\n",
            "Epoch: 2/10... Step: 4576... Loss: 2.7745... ppl: 36.3893  Val Loss: 3.5943\n",
            "Epoch: 2/10... Step: 4608... Loss: 2.9202... ppl: 36.2797  Val Loss: 3.5913\n",
            "Epoch: 2/10... Step: 4640... Loss: 3.7424... ppl: 36.0650  Val Loss: 3.5853\n",
            "Epoch: 2/10... Step: 4672... Loss: 3.6632... ppl: 35.9911  Val Loss: 3.5833\n",
            "Epoch: 3/10... Step: 4704... Loss: 3.0389... ppl: 35.9353  Val Loss: 3.5817\n",
            "Epoch: 3/10... Step: 4736... Loss: 4.2057... ppl: 36.2304  Val Loss: 3.5899\n",
            "Epoch: 3/10... Step: 4768... Loss: 3.0901... ppl: 36.0903  Val Loss: 3.5860\n",
            "Epoch: 3/10... Step: 4800... Loss: 2.6122... ppl: 36.3156  Val Loss: 3.5922\n",
            "Epoch: 3/10... Step: 4832... Loss: 3.3392... ppl: 36.2260  Val Loss: 3.5898\n",
            "Epoch: 3/10... Step: 4864... Loss: 3.7309... ppl: 35.9643  Val Loss: 3.5825\n",
            "Epoch: 3/10... Step: 4896... Loss: 2.5389... ppl: 35.9345  Val Loss: 3.5817\n",
            "Epoch: 3/10... Step: 4928... Loss: 2.0659... ppl: 35.9886  Val Loss: 3.5832\n",
            "Epoch: 3/10... Step: 4960... Loss: 3.2252... ppl: 35.8822  Val Loss: 3.5802\n",
            "Epoch: 3/10... Step: 4992... Loss: 3.4376... ppl: 36.2916  Val Loss: 3.5916\n",
            "Epoch: 3/10... Step: 5024... Loss: 3.4489... ppl: 35.8546  Val Loss: 3.5795\n",
            "Epoch: 3/10... Step: 5056... Loss: 2.9950... ppl: 35.6531  Val Loss: 3.5738\n",
            "Epoch: 3/10... Step: 5088... Loss: 2.7775... ppl: 35.7014  Val Loss: 3.5752\n",
            "Epoch: 3/10... Step: 5120... Loss: 3.6935... ppl: 35.7754  Val Loss: 3.5773\n",
            "Epoch: 3/10... Step: 5152... Loss: 2.8962... ppl: 35.8100  Val Loss: 3.5782\n",
            "Epoch: 3/10... Step: 5184... Loss: 2.8561... ppl: 35.6775  Val Loss: 3.5745\n",
            "Epoch: 3/10... Step: 5216... Loss: 2.9423... ppl: 35.4179  Val Loss: 3.5672\n",
            "Epoch: 3/10... Step: 5248... Loss: 3.0272... ppl: 35.3807  Val Loss: 3.5662\n",
            "Epoch: 3/10... Step: 5280... Loss: 3.2728... ppl: 35.6888  Val Loss: 3.5748\n",
            "Epoch: 3/10... Step: 5312... Loss: 3.5174... ppl: 35.5058  Val Loss: 3.5697\n",
            "Epoch: 3/10... Step: 5344... Loss: 3.7955... ppl: 35.4235  Val Loss: 3.5674\n",
            "Epoch: 3/10... Step: 5376... Loss: 4.1074... ppl: 35.4977  Val Loss: 3.5695\n",
            "Epoch: 3/10... Step: 5408... Loss: 3.8345... ppl: 35.3987  Val Loss: 3.5667\n",
            "Epoch: 3/10... Step: 5440... Loss: 4.3604... ppl: 35.5301  Val Loss: 3.5704\n",
            "Epoch: 3/10... Step: 5472... Loss: 3.7760... ppl: 35.2147  Val Loss: 3.5615\n",
            "Epoch: 3/10... Step: 5504... Loss: 3.6714... ppl: 35.3052  Val Loss: 3.5640\n",
            "Epoch: 3/10... Step: 5536... Loss: 3.3058... ppl: 35.5417  Val Loss: 3.5707\n",
            "Epoch: 3/10... Step: 5568... Loss: 3.0940... ppl: 35.1789  Val Loss: 3.5604\n",
            "Epoch: 3/10... Step: 5600... Loss: 3.3895... ppl: 35.2366  Val Loss: 3.5621\n",
            "Epoch: 3/10... Step: 5632... Loss: 3.3663... ppl: 35.3091  Val Loss: 3.5641\n",
            "Epoch: 3/10... Step: 5664... Loss: 3.1801... ppl: 35.2886  Val Loss: 3.5636\n",
            "Epoch: 3/10... Step: 5696... Loss: 3.6676... ppl: 35.0842  Val Loss: 3.5578\n",
            "Epoch: 3/10... Step: 5728... Loss: 3.1981... ppl: 35.3682  Val Loss: 3.5658\n",
            "Epoch: 3/10... Step: 5760... Loss: 3.8276... ppl: 35.0739  Val Loss: 3.5575\n",
            "Epoch: 3/10... Step: 5792... Loss: 4.0660... ppl: 34.9394  Val Loss: 3.5536\n",
            "Epoch: 3/10... Step: 5824... Loss: 3.1963... ppl: 35.0806  Val Loss: 3.5576\n",
            "Epoch: 3/10... Step: 5856... Loss: 3.3318... ppl: 34.8108  Val Loss: 3.5499\n",
            "Epoch: 3/10... Step: 5888... Loss: 3.1620... ppl: 34.8080  Val Loss: 3.5498\n",
            "Epoch: 3/10... Step: 5920... Loss: 3.5196... ppl: 34.7637  Val Loss: 3.5486\n",
            "Epoch: 3/10... Step: 5952... Loss: 3.8576... ppl: 34.6664  Val Loss: 3.5458\n",
            "Epoch: 3/10... Step: 5984... Loss: 3.2138... ppl: 34.6410  Val Loss: 3.5450\n",
            "Epoch: 3/10... Step: 6016... Loss: 3.7742... ppl: 34.5932  Val Loss: 3.5437\n",
            "Epoch: 3/10... Step: 6048... Loss: 3.3876... ppl: 34.8321  Val Loss: 3.5505\n",
            "Epoch: 3/10... Step: 6080... Loss: 2.6816... ppl: 34.4369  Val Loss: 3.5391\n",
            "Epoch: 3/10... Step: 6112... Loss: 3.3519... ppl: 34.5323  Val Loss: 3.5419\n",
            "Epoch: 3/10... Step: 6144... Loss: 3.0611... ppl: 34.3692  Val Loss: 3.5372\n",
            "Epoch: 3/10... Step: 6176... Loss: 3.1843... ppl: 34.4930  Val Loss: 3.5408\n",
            "Epoch: 3/10... Step: 6208... Loss: 3.5555... ppl: 34.5553  Val Loss: 3.5426\n",
            "Epoch: 3/10... Step: 6240... Loss: 2.6947... ppl: 34.4204  Val Loss: 3.5387\n",
            "Epoch: 3/10... Step: 6272... Loss: 2.6947... ppl: 34.5635  Val Loss: 3.5428\n",
            "Epoch: 3/10... Step: 6304... Loss: 3.1231... ppl: 34.2779  Val Loss: 3.5345\n",
            "Epoch: 3/10... Step: 6336... Loss: 3.4493... ppl: 34.2278  Val Loss: 3.5330\n",
            "Epoch: 3/10... Step: 6368... Loss: 3.0609... ppl: 34.1999  Val Loss: 3.5322\n",
            "Epoch: 3/10... Step: 6400... Loss: 3.7137... ppl: 34.2703  Val Loss: 3.5343\n",
            "Epoch: 3/10... Step: 6432... Loss: 3.6733... ppl: 34.1881  Val Loss: 3.5319\n",
            "Epoch: 3/10... Step: 6464... Loss: 3.4892... ppl: 34.2729  Val Loss: 3.5344\n",
            "Epoch: 3/10... Step: 6496... Loss: 2.7477... ppl: 34.2682  Val Loss: 3.5342\n",
            "Epoch: 3/10... Step: 6528... Loss: 3.1631... ppl: 34.1783  Val Loss: 3.5316\n",
            "Epoch: 3/10... Step: 6560... Loss: 3.2952... ppl: 33.9903  Val Loss: 3.5261\n",
            "Epoch: 3/10... Step: 6592... Loss: 2.5748... ppl: 34.1750  Val Loss: 3.5315\n",
            "Epoch: 3/10... Step: 6624... Loss: 3.7429... ppl: 33.9810  Val Loss: 3.5258\n",
            "Epoch: 3/10... Step: 6656... Loss: 3.0154... ppl: 33.9121  Val Loss: 3.5238\n",
            "Epoch: 3/10... Step: 6688... Loss: 2.7420... ppl: 34.0912  Val Loss: 3.5290\n",
            "Epoch: 3/10... Step: 6720... Loss: 2.5862... ppl: 33.9243  Val Loss: 3.5241\n",
            "Epoch: 3/10... Step: 6752... Loss: 3.4995... ppl: 33.9391  Val Loss: 3.5246\n",
            "Epoch: 3/10... Step: 6784... Loss: 3.1914... ppl: 34.0031  Val Loss: 3.5265\n",
            "Epoch: 3/10... Step: 6816... Loss: 2.7872... ppl: 34.1078  Val Loss: 3.5295\n",
            "Epoch: 3/10... Step: 6848... Loss: 2.9025... ppl: 33.7663  Val Loss: 3.5195\n",
            "Epoch: 3/10... Step: 6880... Loss: 3.0098... ppl: 33.8886  Val Loss: 3.5231\n",
            "Epoch: 3/10... Step: 6912... Loss: 4.1260... ppl: 34.0396  Val Loss: 3.5275\n",
            "Epoch: 3/10... Step: 6944... Loss: 3.1959... ppl: 34.1943  Val Loss: 3.5321\n",
            "Epoch: 3/10... Step: 6976... Loss: 3.3454... ppl: 33.8549  Val Loss: 3.5221\n",
            "Epoch: 3/10... Step: 7008... Loss: 3.3922... ppl: 33.7956  Val Loss: 3.5203\n",
            "Epoch: 4/10... Step: 7040... Loss: 3.4787... ppl: 33.6584  Val Loss: 3.5163\n",
            "Epoch: 4/10... Step: 7072... Loss: 2.9801... ppl: 33.9709  Val Loss: 3.5255\n",
            "Epoch: 4/10... Step: 7104... Loss: 3.2800... ppl: 33.7430  Val Loss: 3.5188\n",
            "Epoch: 4/10... Step: 7136... Loss: 2.7810... ppl: 33.9761  Val Loss: 3.5257\n",
            "Epoch: 4/10... Step: 7168... Loss: 3.4963... ppl: 34.0907  Val Loss: 3.5290\n",
            "Epoch: 4/10... Step: 7200... Loss: 2.6738... ppl: 33.9245  Val Loss: 3.5241\n",
            "Epoch: 4/10... Step: 7232... Loss: 3.0372... ppl: 33.9341  Val Loss: 3.5244\n",
            "Epoch: 4/10... Step: 7264... Loss: 4.3198... ppl: 33.9001  Val Loss: 3.5234\n",
            "Epoch: 4/10... Step: 7296... Loss: 3.5948... ppl: 33.9647  Val Loss: 3.5253\n",
            "Epoch: 4/10... Step: 7328... Loss: 2.6139... ppl: 34.2127  Val Loss: 3.5326\n",
            "Epoch: 4/10... Step: 7360... Loss: 2.8747... ppl: 33.9411  Val Loss: 3.5246\n",
            "Epoch: 4/10... Step: 7392... Loss: 3.1296... ppl: 33.8217  Val Loss: 3.5211\n",
            "Epoch: 4/10... Step: 7424... Loss: 2.6709... ppl: 33.6255  Val Loss: 3.5153\n",
            "Epoch: 4/10... Step: 7456... Loss: 2.7630... ppl: 33.8563  Val Loss: 3.5221\n",
            "Epoch: 4/10... Step: 7488... Loss: 3.5197... ppl: 33.9330  Val Loss: 3.5244\n",
            "Epoch: 4/10... Step: 7520... Loss: 1.9849... ppl: 34.0151  Val Loss: 3.5268\n",
            "Epoch: 4/10... Step: 7552... Loss: 3.3183... ppl: 33.7926  Val Loss: 3.5202\n",
            "Epoch: 4/10... Step: 7584... Loss: 2.6120... ppl: 33.8164  Val Loss: 3.5209\n",
            "Epoch: 4/10... Step: 7616... Loss: 3.0070... ppl: 33.8319  Val Loss: 3.5214\n",
            "Epoch: 4/10... Step: 7648... Loss: 3.4960... ppl: 33.7968  Val Loss: 3.5204\n",
            "Epoch: 4/10... Step: 7680... Loss: 2.8388... ppl: 33.8457  Val Loss: 3.5218\n",
            "Epoch: 4/10... Step: 7712... Loss: 2.7631... ppl: 33.8927  Val Loss: 3.5232\n",
            "Epoch: 4/10... Step: 7744... Loss: 3.2505... ppl: 33.8718  Val Loss: 3.5226\n",
            "Epoch: 4/10... Step: 7776... Loss: 3.7493... ppl: 34.0955  Val Loss: 3.5292\n",
            "Epoch: 4/10... Step: 7808... Loss: 3.5084... ppl: 33.7037  Val Loss: 3.5176\n",
            "Epoch: 4/10... Step: 7840... Loss: 2.0866... ppl: 33.7797  Val Loss: 3.5199\n",
            "Epoch: 4/10... Step: 7872... Loss: 3.6077... ppl: 33.9738  Val Loss: 3.5256\n",
            "Epoch: 4/10... Step: 7904... Loss: 2.3272... ppl: 33.7507  Val Loss: 3.5190\n",
            "Epoch: 4/10... Step: 7936... Loss: 3.1566... ppl: 33.8072  Val Loss: 3.5207\n",
            "Epoch: 4/10... Step: 7968... Loss: 3.7764... ppl: 33.7634  Val Loss: 3.5194\n",
            "Epoch: 4/10... Step: 8000... Loss: 2.9711... ppl: 34.0098  Val Loss: 3.5266\n",
            "Epoch: 4/10... Step: 8032... Loss: 3.3482... ppl: 33.9204  Val Loss: 3.5240\n",
            "Epoch: 4/10... Step: 8064... Loss: 2.5684... ppl: 34.0141  Val Loss: 3.5268\n",
            "Epoch: 4/10... Step: 8096... Loss: 3.0871... ppl: 33.8743  Val Loss: 3.5227\n",
            "Epoch: 4/10... Step: 8128... Loss: 2.2880... ppl: 33.7794  Val Loss: 3.5199\n",
            "Epoch: 4/10... Step: 8160... Loss: 2.9260... ppl: 33.7664  Val Loss: 3.5195\n",
            "Epoch: 4/10... Step: 8192... Loss: 3.2412... ppl: 33.7783  Val Loss: 3.5198\n",
            "Epoch: 4/10... Step: 8224... Loss: 3.0643... ppl: 33.6106  Val Loss: 3.5148\n",
            "Epoch: 4/10... Step: 8256... Loss: 2.6479... ppl: 33.5447  Val Loss: 3.5129\n",
            "Epoch: 4/10... Step: 8288... Loss: 3.0829... ppl: 33.6219  Val Loss: 3.5152\n",
            "Epoch: 4/10... Step: 8320... Loss: 3.1793... ppl: 33.4512  Val Loss: 3.5101\n",
            "Epoch: 4/10... Step: 8352... Loss: 2.8935... ppl: 33.4726  Val Loss: 3.5107\n",
            "Epoch: 4/10... Step: 8384... Loss: 2.6961... ppl: 33.8965  Val Loss: 3.5233\n",
            "Epoch: 4/10... Step: 8416... Loss: 2.7846... ppl: 33.4528  Val Loss: 3.5101\n",
            "Epoch: 4/10... Step: 8448... Loss: 4.0183... ppl: 33.5169  Val Loss: 3.5120\n",
            "Epoch: 4/10... Step: 8480... Loss: 2.8277... ppl: 33.4669  Val Loss: 3.5106\n",
            "Epoch: 4/10... Step: 8512... Loss: 2.4755... ppl: 33.4219  Val Loss: 3.5092\n",
            "Epoch: 4/10... Step: 8544... Loss: 3.0584... ppl: 33.5556  Val Loss: 3.5132\n",
            "Epoch: 4/10... Step: 8576... Loss: 2.9431... ppl: 33.5468  Val Loss: 3.5129\n",
            "Epoch: 4/10... Step: 8608... Loss: 3.5072... ppl: 33.6395  Val Loss: 3.5157\n",
            "Epoch: 4/10... Step: 8640... Loss: 3.2672... ppl: 33.4891  Val Loss: 3.5112\n",
            "Epoch: 4/10... Step: 8672... Loss: 2.6272... ppl: 33.3435  Val Loss: 3.5069\n",
            "Epoch: 4/10... Step: 8704... Loss: 3.7044... ppl: 33.2929  Val Loss: 3.5053\n",
            "Epoch: 4/10... Step: 8736... Loss: 2.9565... ppl: 33.3763  Val Loss: 3.5078\n",
            "Epoch: 4/10... Step: 8768... Loss: 2.6255... ppl: 33.3862  Val Loss: 3.5081\n",
            "Epoch: 4/10... Step: 8800... Loss: 3.5006... ppl: 33.4924  Val Loss: 3.5113\n",
            "Epoch: 4/10... Step: 8832... Loss: 3.0042... ppl: 33.4364  Val Loss: 3.5096\n",
            "Epoch: 4/10... Step: 8864... Loss: 3.2144... ppl: 33.6484  Val Loss: 3.5160\n",
            "Epoch: 4/10... Step: 8896... Loss: 2.6678... ppl: 33.4209  Val Loss: 3.5092\n",
            "Epoch: 4/10... Step: 8928... Loss: 2.8080... ppl: 33.4960  Val Loss: 3.5114\n",
            "Epoch: 4/10... Step: 8960... Loss: 2.3489... ppl: 33.2686  Val Loss: 3.5046\n",
            "Epoch: 4/10... Step: 8992... Loss: 3.0552... ppl: 33.2317  Val Loss: 3.5035\n",
            "Epoch: 4/10... Step: 9024... Loss: 3.5104... ppl: 33.5706  Val Loss: 3.5136\n",
            "Epoch: 4/10... Step: 9056... Loss: 3.0033... ppl: 33.2501  Val Loss: 3.5041\n",
            "Epoch: 4/10... Step: 9088... Loss: 3.2950... ppl: 33.3150  Val Loss: 3.5060\n",
            "Epoch: 4/10... Step: 9120... Loss: 2.5947... ppl: 33.3895  Val Loss: 3.5082\n",
            "Epoch: 4/10... Step: 9152... Loss: 2.9470... ppl: 33.4965  Val Loss: 3.5114\n",
            "Epoch: 4/10... Step: 9184... Loss: 3.3625... ppl: 33.2541  Val Loss: 3.5042\n",
            "Epoch: 4/10... Step: 9216... Loss: 2.4216... ppl: 33.3401  Val Loss: 3.5068\n",
            "Epoch: 4/10... Step: 9248... Loss: 3.6211... ppl: 33.3821  Val Loss: 3.5080\n",
            "Epoch: 4/10... Step: 9280... Loss: 2.6688... ppl: 33.8239  Val Loss: 3.5212\n",
            "Epoch: 4/10... Step: 9312... Loss: 3.3672... ppl: 33.3793  Val Loss: 3.5079\n",
            "Epoch: 4/10... Step: 9344... Loss: 3.2074... ppl: 33.2444  Val Loss: 3.5039\n",
            "Epoch: 5/10... Step: 9376... Loss: 2.0323... ppl: 33.1657  Val Loss: 3.5015\n",
            "Epoch: 5/10... Step: 9408... Loss: 3.1909... ppl: 33.4069  Val Loss: 3.5088\n",
            "Epoch: 5/10... Step: 9440... Loss: 2.7797... ppl: 33.1490  Val Loss: 3.5010\n",
            "Epoch: 5/10... Step: 9472... Loss: 2.7595... ppl: 33.3903  Val Loss: 3.5083\n",
            "Epoch: 5/10... Step: 9504... Loss: 3.0762... ppl: 33.5093  Val Loss: 3.5118\n",
            "Epoch: 5/10... Step: 9536... Loss: 2.2126... ppl: 33.3667  Val Loss: 3.5076\n",
            "Epoch: 5/10... Step: 9568... Loss: 1.8932... ppl: 33.5953  Val Loss: 3.5144\n",
            "Epoch: 5/10... Step: 9600... Loss: 2.9922... ppl: 33.4925  Val Loss: 3.5113\n",
            "Epoch: 5/10... Step: 9632... Loss: 3.1558... ppl: 33.6370  Val Loss: 3.5156\n",
            "Epoch: 5/10... Step: 9664... Loss: 3.2265... ppl: 33.6318  Val Loss: 3.5155\n",
            "Epoch: 5/10... Step: 9696... Loss: 2.2869... ppl: 33.6625  Val Loss: 3.5164\n",
            "Epoch: 5/10... Step: 9728... Loss: 3.1715... ppl: 33.4709  Val Loss: 3.5107\n",
            "Epoch: 5/10... Step: 9760... Loss: 2.4828... ppl: 33.2957  Val Loss: 3.5054\n",
            "Epoch: 5/10... Step: 9792... Loss: 3.3420... ppl: 33.4449  Val Loss: 3.5099\n",
            "Epoch: 5/10... Step: 9824... Loss: 3.2822... ppl: 33.5601  Val Loss: 3.5133\n",
            "Epoch: 5/10... Step: 9856... Loss: 2.0289... ppl: 33.7638  Val Loss: 3.5194\n",
            "Epoch: 5/10... Step: 9888... Loss: 3.1957... ppl: 33.4563  Val Loss: 3.5102\n",
            "Epoch: 5/10... Step: 9920... Loss: 2.9041... ppl: 33.5177  Val Loss: 3.5121\n",
            "Epoch: 5/10... Step: 9952... Loss: 3.2938... ppl: 33.4229  Val Loss: 3.5092\n",
            "Epoch: 5/10... Step: 9984... Loss: 3.1827... ppl: 33.5581  Val Loss: 3.5133\n",
            "Epoch: 5/10... Step: 10016... Loss: 2.8002... ppl: 33.6222  Val Loss: 3.5152\n",
            "Epoch: 5/10... Step: 10048... Loss: 2.6903... ppl: 33.6911  Val Loss: 3.5172\n",
            "Epoch: 5/10... Step: 10080... Loss: 2.8424... ppl: 33.7548  Val Loss: 3.5191\n",
            "Epoch: 5/10... Step: 10112... Loss: 3.7818... ppl: 33.8111  Val Loss: 3.5208\n",
            "Epoch: 5/10... Step: 10144... Loss: 2.7630... ppl: 33.6125  Val Loss: 3.5149\n",
            "Epoch: 5/10... Step: 10176... Loss: 2.5212... ppl: 33.5780  Val Loss: 3.5139\n",
            "Epoch: 5/10... Step: 10208... Loss: 3.3385... ppl: 33.6992  Val Loss: 3.5175\n",
            "Epoch: 5/10... Step: 10240... Loss: 3.1982... ppl: 33.6943  Val Loss: 3.5173\n",
            "Epoch: 5/10... Step: 10272... Loss: 3.6203... ppl: 33.5694  Val Loss: 3.5136\n",
            "Epoch: 5/10... Step: 10304... Loss: 3.6645... ppl: 33.4919  Val Loss: 3.5113\n",
            "Epoch: 5/10... Step: 10336... Loss: 2.9679... ppl: 33.8120  Val Loss: 3.5208\n",
            "Epoch: 5/10... Step: 10368... Loss: 2.9081... ppl: 33.7140  Val Loss: 3.5179\n",
            "Epoch: 5/10... Step: 10400... Loss: 3.0207... ppl: 33.6753  Val Loss: 3.5168\n",
            "Epoch: 5/10... Step: 10432... Loss: 2.9265... ppl: 33.8062  Val Loss: 3.5206\n",
            "Epoch: 5/10... Step: 10464... Loss: 3.0188... ppl: 33.6208  Val Loss: 3.5151\n",
            "Epoch: 5/10... Step: 10496... Loss: 2.5837... ppl: 33.6498  Val Loss: 3.5160\n",
            "Epoch: 5/10... Step: 10528... Loss: 3.2956... ppl: 33.8342  Val Loss: 3.5215\n",
            "Epoch: 5/10... Step: 10560... Loss: 3.6614... ppl: 33.5336  Val Loss: 3.5125\n",
            "Epoch: 5/10... Step: 10592... Loss: 2.8682... ppl: 33.5039  Val Loss: 3.5117\n",
            "Epoch: 5/10... Step: 10624... Loss: 2.9365... ppl: 33.6962  Val Loss: 3.5174\n",
            "Epoch: 5/10... Step: 10656... Loss: 2.9591... ppl: 33.4946  Val Loss: 3.5114\n",
            "Epoch: 5/10... Step: 10688... Loss: 2.7699... ppl: 33.4070  Val Loss: 3.5088\n",
            "Epoch: 5/10... Step: 10720... Loss: 2.8010... ppl: 33.6956  Val Loss: 3.5174\n",
            "Epoch: 5/10... Step: 10752... Loss: 2.6885... ppl: 33.5765  Val Loss: 3.5138\n",
            "Epoch: 5/10... Step: 10784... Loss: 3.4577... ppl: 33.4567  Val Loss: 3.5103\n",
            "Epoch: 5/10... Step: 10816... Loss: 3.0706... ppl: 33.5589  Val Loss: 3.5133\n",
            "Epoch: 5/10... Step: 10848... Loss: 3.3341... ppl: 33.5013  Val Loss: 3.5116\n",
            "Epoch: 5/10... Step: 10880... Loss: 2.8276... ppl: 33.5803  Val Loss: 3.5139\n",
            "Epoch: 5/10... Step: 10912... Loss: 2.4586... ppl: 33.5193  Val Loss: 3.5121\n",
            "Epoch: 5/10... Step: 10944... Loss: 3.0189... ppl: 33.6313  Val Loss: 3.5155\n",
            "Epoch: 5/10... Step: 10976... Loss: 3.2386... ppl: 33.6069  Val Loss: 3.5147\n",
            "Epoch: 5/10... Step: 11008... Loss: 3.1710... ppl: 33.4226  Val Loss: 3.5092\n",
            "Epoch: 5/10... Step: 11040... Loss: 3.6089... ppl: 33.3623  Val Loss: 3.5074\n",
            "Epoch: 5/10... Step: 11072... Loss: 3.4578... ppl: 33.4841  Val Loss: 3.5111\n",
            "Epoch: 5/10... Step: 11104... Loss: 3.0997... ppl: 33.5380  Val Loss: 3.5127\n",
            "Epoch: 5/10... Step: 11136... Loss: 3.6129... ppl: 33.4772  Val Loss: 3.5109\n",
            "Epoch: 5/10... Step: 11168... Loss: 2.4198... ppl: 33.7019  Val Loss: 3.5176\n",
            "Epoch: 5/10... Step: 11200... Loss: 2.8680... ppl: 33.8994  Val Loss: 3.5234\n",
            "Epoch: 5/10... Step: 11232... Loss: 2.8222... ppl: 33.6408  Val Loss: 3.5157\n",
            "Epoch: 5/10... Step: 11264... Loss: 2.0565... ppl: 33.6354  Val Loss: 3.5156\n",
            "Epoch: 5/10... Step: 11296... Loss: 2.6702... ppl: 33.6532  Val Loss: 3.5161\n",
            "Epoch: 5/10... Step: 11328... Loss: 3.7120... ppl: 33.4587  Val Loss: 3.5103\n",
            "Epoch: 5/10... Step: 11360... Loss: 2.6759... ppl: 33.8095  Val Loss: 3.5207\n",
            "Epoch: 5/10... Step: 11392... Loss: 2.5840... ppl: 33.5127  Val Loss: 3.5119\n",
            "Epoch: 5/10... Step: 11424... Loss: 3.3957... ppl: 33.4394  Val Loss: 3.5097\n",
            "Epoch: 5/10... Step: 11456... Loss: 2.2323... ppl: 33.5614  Val Loss: 3.5134\n",
            "Epoch: 5/10... Step: 11488... Loss: 2.7768... ppl: 33.7085  Val Loss: 3.5177\n",
            "Epoch: 5/10... Step: 11520... Loss: 2.3351... ppl: 33.5547  Val Loss: 3.5132\n",
            "Epoch: 5/10... Step: 11552... Loss: 3.8202... ppl: 33.4697  Val Loss: 3.5106\n",
            "Epoch: 5/10... Step: 11584... Loss: 2.9175... ppl: 33.5202  Val Loss: 3.5121\n",
            "Epoch: 5/10... Step: 11616... Loss: 2.3851... ppl: 34.0339  Val Loss: 3.5274\n",
            "Epoch: 5/10... Step: 11648... Loss: 2.8363... ppl: 33.6789  Val Loss: 3.5169\n",
            "Epoch: 5/10... Step: 11680... Loss: 3.2096... ppl: 33.5713  Val Loss: 3.5137\n",
            "Epoch: 5/10... Step: 11712... Loss: 3.1122... ppl: 33.4845  Val Loss: 3.5111\n",
            "Epoch: 6/10... Step: 11744... Loss: 3.4609... ppl: 33.6381  Val Loss: 3.5157\n",
            "Epoch: 6/10... Step: 11776... Loss: 2.8159... ppl: 33.5117  Val Loss: 3.5119\n",
            "Epoch: 6/10... Step: 11808... Loss: 2.6455... ppl: 33.5405  Val Loss: 3.5128\n",
            "Epoch: 6/10... Step: 11840... Loss: 3.2696... ppl: 33.7197  Val Loss: 3.5181\n",
            "Epoch: 6/10... Step: 11872... Loss: 2.8669... ppl: 33.5916  Val Loss: 3.5143\n",
            "Epoch: 6/10... Step: 11904... Loss: 3.0799... ppl: 33.8121  Val Loss: 3.5208\n",
            "Epoch: 6/10... Step: 11936... Loss: 2.9493... ppl: 33.7705  Val Loss: 3.5196\n",
            "Epoch: 6/10... Step: 11968... Loss: 2.7317... ppl: 33.9814  Val Loss: 3.5258\n",
            "Epoch: 6/10... Step: 12000... Loss: 2.5206... ppl: 33.8236  Val Loss: 3.5212\n",
            "Epoch: 6/10... Step: 12032... Loss: 2.9594... ppl: 34.1892  Val Loss: 3.5319\n",
            "Epoch: 6/10... Step: 12064... Loss: 3.0981... ppl: 33.8498  Val Loss: 3.5219\n",
            "Epoch: 6/10... Step: 12096... Loss: 3.6823... ppl: 33.6515  Val Loss: 3.5161\n",
            "Epoch: 6/10... Step: 12128... Loss: 2.4709... ppl: 33.6879  Val Loss: 3.5171\n",
            "Epoch: 6/10... Step: 12160... Loss: 2.9944... ppl: 33.9249  Val Loss: 3.5242\n",
            "Epoch: 6/10... Step: 12192... Loss: 2.7264... ppl: 34.2242  Val Loss: 3.5329\n",
            "Epoch: 6/10... Step: 12224... Loss: 2.5843... ppl: 33.9746  Val Loss: 3.5256\n",
            "Epoch: 6/10... Step: 12256... Loss: 2.6410... ppl: 33.9758  Val Loss: 3.5256\n",
            "Epoch: 6/10... Step: 12288... Loss: 2.7658... ppl: 33.8970  Val Loss: 3.5233\n",
            "Epoch: 6/10... Step: 12320... Loss: 2.9024... ppl: 34.0558  Val Loss: 3.5280\n",
            "Epoch: 6/10... Step: 12352... Loss: 2.7072... ppl: 34.0607  Val Loss: 3.5281\n",
            "Epoch: 6/10... Step: 12384... Loss: 2.9780... ppl: 34.1565  Val Loss: 3.5310\n",
            "Epoch: 6/10... Step: 12416... Loss: 2.9506... ppl: 34.2132  Val Loss: 3.5326\n",
            "Epoch: 6/10... Step: 12448... Loss: 2.8879... ppl: 34.1272  Val Loss: 3.5301\n",
            "Epoch: 6/10... Step: 12480... Loss: 2.6040... ppl: 34.1402  Val Loss: 3.5305\n",
            "Epoch: 6/10... Step: 12512... Loss: 2.7865... ppl: 33.9564  Val Loss: 3.5251\n",
            "Epoch: 6/10... Step: 12544... Loss: 2.8622... ppl: 34.1858  Val Loss: 3.5318\n",
            "Epoch: 6/10... Step: 12576... Loss: 2.4421... ppl: 34.4438  Val Loss: 3.5393\n",
            "Epoch: 6/10... Step: 12608... Loss: 3.1866... ppl: 34.1180  Val Loss: 3.5298\n",
            "Epoch: 6/10... Step: 12640... Loss: 2.5685... ppl: 33.9622  Val Loss: 3.5252\n",
            "Epoch: 6/10... Step: 12672... Loss: 3.0563... ppl: 34.2057  Val Loss: 3.5324\n",
            "Epoch: 6/10... Step: 12704... Loss: 2.5251... ppl: 34.2243  Val Loss: 3.5329\n",
            "Epoch: 6/10... Step: 12736... Loss: 3.2172... ppl: 34.0734  Val Loss: 3.5285\n",
            "Epoch: 6/10... Step: 12768... Loss: 2.6078... ppl: 34.4802  Val Loss: 3.5404\n",
            "Epoch: 6/10... Step: 12800... Loss: 2.8702... ppl: 34.2208  Val Loss: 3.5328\n",
            "Epoch: 6/10... Step: 12832... Loss: 2.9182... ppl: 34.2607  Val Loss: 3.5340\n",
            "Epoch: 6/10... Step: 12864... Loss: 2.9880... ppl: 34.2875  Val Loss: 3.5348\n",
            "Epoch: 6/10... Step: 12896... Loss: 3.1452... ppl: 34.0258  Val Loss: 3.5271\n",
            "Epoch: 6/10... Step: 12928... Loss: 2.7451... ppl: 34.0221  Val Loss: 3.5270\n",
            "Epoch: 6/10... Step: 12960... Loss: 2.8964... ppl: 34.1287  Val Loss: 3.5301\n",
            "Epoch: 6/10... Step: 12992... Loss: 3.0966... ppl: 34.1108  Val Loss: 3.5296\n",
            "Epoch: 6/10... Step: 13024... Loss: 2.3891... ppl: 33.9575  Val Loss: 3.5251\n",
            "Epoch: 6/10... Step: 13056... Loss: 2.7083... ppl: 34.1605  Val Loss: 3.5311\n",
            "Epoch: 6/10... Step: 13088... Loss: 2.6163... ppl: 34.3633  Val Loss: 3.5370\n",
            "Epoch: 6/10... Step: 13120... Loss: 3.0927... ppl: 33.9603  Val Loss: 3.5252\n",
            "Epoch: 6/10... Step: 13152... Loss: 2.8134... ppl: 34.2234  Val Loss: 3.5329\n",
            "Epoch: 6/10... Step: 13184... Loss: 3.3291... ppl: 34.1004  Val Loss: 3.5293\n",
            "Epoch: 6/10... Step: 13216... Loss: 3.3879... ppl: 33.9988  Val Loss: 3.5263\n",
            "Epoch: 6/10... Step: 13248... Loss: 3.3921... ppl: 34.0756  Val Loss: 3.5286\n",
            "Epoch: 6/10... Step: 13280... Loss: 3.2390... ppl: 34.1539  Val Loss: 3.5309\n",
            "Epoch: 6/10... Step: 13312... Loss: 2.8991... ppl: 34.2221  Val Loss: 3.5329\n",
            "Epoch: 6/10... Step: 13344... Loss: 2.5012... ppl: 34.1707  Val Loss: 3.5314\n",
            "Epoch: 6/10... Step: 13376... Loss: 2.5651... ppl: 33.9954  Val Loss: 3.5262\n",
            "Epoch: 6/10... Step: 13408... Loss: 2.1738... ppl: 34.1569  Val Loss: 3.5310\n",
            "Epoch: 6/10... Step: 13440... Loss: 3.2604... ppl: 34.0984  Val Loss: 3.5292\n",
            "Epoch: 6/10... Step: 13472... Loss: 3.1368... ppl: 34.0202  Val Loss: 3.5270\n",
            "Epoch: 6/10... Step: 13504... Loss: 2.8212... ppl: 34.3188  Val Loss: 3.5357\n",
            "Epoch: 6/10... Step: 13536... Loss: 3.1048... ppl: 34.4917  Val Loss: 3.5407\n",
            "Epoch: 6/10... Step: 13568... Loss: 2.7350... ppl: 34.2217  Val Loss: 3.5329\n",
            "Epoch: 6/10... Step: 13600... Loss: 2.8416... ppl: 34.1459  Val Loss: 3.5306\n",
            "Epoch: 6/10... Step: 13632... Loss: 3.7399... ppl: 34.4250  Val Loss: 3.5388\n",
            "Epoch: 6/10... Step: 13664... Loss: 2.8523... ppl: 34.1663  Val Loss: 3.5312\n",
            "Epoch: 6/10... Step: 13696... Loss: 3.2410... ppl: 34.2543  Val Loss: 3.5338\n",
            "Epoch: 6/10... Step: 13728... Loss: 2.5069... ppl: 34.3435  Val Loss: 3.5364\n",
            "Epoch: 6/10... Step: 13760... Loss: 3.1425... ppl: 34.1471  Val Loss: 3.5307\n",
            "Epoch: 6/10... Step: 13792... Loss: 3.0805... ppl: 34.3306  Val Loss: 3.5360\n",
            "Epoch: 6/10... Step: 13824... Loss: 2.4523... ppl: 34.4769  Val Loss: 3.5403\n",
            "Epoch: 6/10... Step: 13856... Loss: 3.1018... ppl: 34.4565  Val Loss: 3.5397\n",
            "Epoch: 6/10... Step: 13888... Loss: 2.8448... ppl: 34.1681  Val Loss: 3.5313\n",
            "Epoch: 6/10... Step: 13920... Loss: 3.2647... ppl: 34.2710  Val Loss: 3.5343\n",
            "Epoch: 6/10... Step: 13952... Loss: 2.1357... ppl: 34.5796  Val Loss: 3.5433\n",
            "Epoch: 6/10... Step: 13984... Loss: 3.0613... ppl: 34.5435  Val Loss: 3.5422\n",
            "Epoch: 6/10... Step: 14016... Loss: 2.5841... ppl: 34.3838  Val Loss: 3.5376\n",
            "Epoch: 6/10... Step: 14048... Loss: 2.4904... ppl: 34.1710  Val Loss: 3.5314\n",
            "Epoch: 7/10... Step: 14080... Loss: 2.9629... ppl: 34.2911  Val Loss: 3.5349\n",
            "Epoch: 7/10... Step: 14112... Loss: 2.1054... ppl: 34.3532  Val Loss: 3.5367\n",
            "Epoch: 7/10... Step: 14144... Loss: 2.6740... ppl: 34.1526  Val Loss: 3.5308\n",
            "Epoch: 7/10... Step: 14176... Loss: 2.6991... ppl: 34.4402  Val Loss: 3.5392\n",
            "Epoch: 7/10... Step: 14208... Loss: 2.4524... ppl: 34.3740  Val Loss: 3.5373\n",
            "Epoch: 7/10... Step: 14240... Loss: 2.7781... ppl: 34.3267  Val Loss: 3.5359\n",
            "Epoch: 7/10... Step: 14272... Loss: 2.3806... ppl: 34.4490  Val Loss: 3.5395\n",
            "Epoch: 7/10... Step: 14304... Loss: 2.3711... ppl: 34.5118  Val Loss: 3.5413\n",
            "Epoch: 7/10... Step: 14336... Loss: 2.6353... ppl: 34.4864  Val Loss: 3.5406\n",
            "Epoch: 7/10... Step: 14368... Loss: 3.4167... ppl: 34.9716  Val Loss: 3.5545\n",
            "Epoch: 7/10... Step: 14400... Loss: 2.6506... ppl: 34.7125  Val Loss: 3.5471\n",
            "Epoch: 7/10... Step: 14432... Loss: 2.8409... ppl: 34.4599  Val Loss: 3.5398\n",
            "Epoch: 7/10... Step: 14464... Loss: 2.9744... ppl: 34.3108  Val Loss: 3.5355\n",
            "Epoch: 7/10... Step: 14496... Loss: 3.1284... ppl: 34.6167  Val Loss: 3.5443\n",
            "Epoch: 7/10... Step: 14528... Loss: 2.1639... ppl: 34.8999  Val Loss: 3.5525\n",
            "Epoch: 7/10... Step: 14560... Loss: 2.5090... ppl: 34.8367  Val Loss: 3.5507\n",
            "Epoch: 7/10... Step: 14592... Loss: 2.4895... ppl: 34.5942  Val Loss: 3.5437\n",
            "Epoch: 7/10... Step: 14624... Loss: 2.1960... ppl: 34.5660  Val Loss: 3.5429\n",
            "Epoch: 7/10... Step: 14656... Loss: 2.9572... ppl: 34.8041  Val Loss: 3.5497\n",
            "Epoch: 7/10... Step: 14688... Loss: 3.0711... ppl: 34.7783  Val Loss: 3.5490\n",
            "Epoch: 7/10... Step: 14720... Loss: 3.6828... ppl: 34.8709  Val Loss: 3.5517\n",
            "Epoch: 7/10... Step: 14752... Loss: 3.0341... ppl: 35.0392  Val Loss: 3.5565\n",
            "Epoch: 7/10... Step: 14784... Loss: 2.1277... ppl: 34.9210  Val Loss: 3.5531\n",
            "Epoch: 7/10... Step: 14816... Loss: 2.6880... ppl: 35.1252  Val Loss: 3.5589\n",
            "Epoch: 7/10... Step: 14848... Loss: 2.6244... ppl: 34.7736  Val Loss: 3.5489\n",
            "Epoch: 7/10... Step: 14880... Loss: 2.7084... ppl: 34.9477  Val Loss: 3.5539\n",
            "Epoch: 7/10... Step: 14912... Loss: 2.5613... ppl: 35.2467  Val Loss: 3.5624\n",
            "Epoch: 7/10... Step: 14944... Loss: 3.0722... ppl: 34.9199  Val Loss: 3.5531\n",
            "Epoch: 7/10... Step: 14976... Loss: 2.5811... ppl: 34.8100  Val Loss: 3.5499\n",
            "Epoch: 7/10... Step: 15008... Loss: 2.9221... ppl: 34.9714  Val Loss: 3.5545\n",
            "Epoch: 7/10... Step: 15040... Loss: 2.1473... ppl: 35.0859  Val Loss: 3.5578\n",
            "Epoch: 7/10... Step: 15072... Loss: 2.9932... ppl: 34.9488  Val Loss: 3.5539\n",
            "Epoch: 7/10... Step: 15104... Loss: 2.9344... ppl: 35.3432  Val Loss: 3.5651\n",
            "Epoch: 7/10... Step: 15136... Loss: 3.1185... ppl: 35.2077  Val Loss: 3.5613\n",
            "Epoch: 7/10... Step: 15168... Loss: 2.3609... ppl: 35.0259  Val Loss: 3.5561\n",
            "Epoch: 7/10... Step: 15200... Loss: 2.7243... ppl: 35.0828  Val Loss: 3.5577\n",
            "Epoch: 7/10... Step: 15232... Loss: 3.0238... ppl: 34.8332  Val Loss: 3.5506\n",
            "Epoch: 7/10... Step: 15264... Loss: 2.7394... ppl: 34.8905  Val Loss: 3.5522\n",
            "Epoch: 7/10... Step: 15296... Loss: 3.0981... ppl: 34.8805  Val Loss: 3.5519\n",
            "Epoch: 7/10... Step: 15328... Loss: 2.6659... ppl: 35.0509  Val Loss: 3.5568\n",
            "Epoch: 7/10... Step: 15360... Loss: 2.7788... ppl: 34.8991  Val Loss: 3.5525\n",
            "Epoch: 7/10... Step: 15392... Loss: 2.7901... ppl: 34.8968  Val Loss: 3.5524\n",
            "Epoch: 7/10... Step: 15424... Loss: 2.2574... ppl: 35.2987  Val Loss: 3.5638\n",
            "Epoch: 7/10... Step: 15456... Loss: 3.2277... ppl: 34.9373  Val Loss: 3.5536\n",
            "Epoch: 7/10... Step: 15488... Loss: 2.7550... ppl: 35.0063  Val Loss: 3.5555\n",
            "Epoch: 7/10... Step: 15520... Loss: 3.0047... ppl: 35.0110  Val Loss: 3.5557\n",
            "Epoch: 7/10... Step: 15552... Loss: 2.4725... ppl: 34.7871  Val Loss: 3.5492\n",
            "Epoch: 7/10... Step: 15584... Loss: 3.2958... ppl: 34.8399  Val Loss: 3.5508\n",
            "Epoch: 7/10... Step: 15616... Loss: 2.1494... ppl: 34.8949  Val Loss: 3.5523\n",
            "Epoch: 7/10... Step: 15648... Loss: 2.8048... ppl: 35.0675  Val Loss: 3.5573\n",
            "Epoch: 7/10... Step: 15680... Loss: 2.7745... ppl: 35.1260  Val Loss: 3.5589\n",
            "Epoch: 7/10... Step: 15712... Loss: 2.1397... ppl: 34.8483  Val Loss: 3.5510\n",
            "Epoch: 7/10... Step: 15744... Loss: 2.8504... ppl: 34.9842  Val Loss: 3.5549\n",
            "Epoch: 7/10... Step: 15776... Loss: 2.8961... ppl: 34.9957  Val Loss: 3.5552\n",
            "Epoch: 7/10... Step: 15808... Loss: 3.1604... ppl: 35.0243  Val Loss: 3.5560\n",
            "Epoch: 7/10... Step: 15840... Loss: 3.4726... ppl: 35.1001  Val Loss: 3.5582\n",
            "Epoch: 7/10... Step: 15872... Loss: 3.3247... ppl: 35.1314  Val Loss: 3.5591\n",
            "Epoch: 7/10... Step: 15904... Loss: 2.4842... ppl: 35.1831  Val Loss: 3.5606\n",
            "Epoch: 7/10... Step: 15936... Loss: 2.7341... ppl: 35.1358  Val Loss: 3.5592\n",
            "Epoch: 7/10... Step: 15968... Loss: 1.9516... ppl: 35.4550  Val Loss: 3.5683\n",
            "Epoch: 7/10... Step: 16000... Loss: 2.8614... ppl: 35.0443  Val Loss: 3.5566\n",
            "Epoch: 7/10... Step: 16032... Loss: 2.5467... ppl: 34.9705  Val Loss: 3.5545\n",
            "Epoch: 7/10... Step: 16064... Loss: 2.3136... ppl: 35.3812  Val Loss: 3.5662\n",
            "Epoch: 7/10... Step: 16096... Loss: 2.1784... ppl: 35.0339  Val Loss: 3.5563\n",
            "Epoch: 7/10... Step: 16128... Loss: 2.4873... ppl: 35.0787  Val Loss: 3.5576\n",
            "Epoch: 7/10... Step: 16160... Loss: 2.7333... ppl: 35.2265  Val Loss: 3.5618\n",
            "Epoch: 7/10... Step: 16192... Loss: 2.6401... ppl: 35.4163  Val Loss: 3.5672\n",
            "Epoch: 7/10... Step: 16224... Loss: 3.1496... ppl: 35.0428  Val Loss: 3.5566\n",
            "Epoch: 7/10... Step: 16256... Loss: 2.5311... ppl: 35.1363  Val Loss: 3.5592\n",
            "Epoch: 7/10... Step: 16288... Loss: 2.7404... ppl: 35.3553  Val Loss: 3.5654\n",
            "Epoch: 7/10... Step: 16320... Loss: 2.3116... ppl: 35.5302  Val Loss: 3.5704\n",
            "Epoch: 7/10... Step: 16352... Loss: 2.5008... ppl: 35.5165  Val Loss: 3.5700\n",
            "Epoch: 7/10... Step: 16384... Loss: 3.5176... ppl: 35.0774  Val Loss: 3.5576\n",
            "Epoch: 8/10... Step: 16416... Loss: 3.2184... ppl: 35.0391  Val Loss: 3.5565\n",
            "Epoch: 8/10... Step: 16448... Loss: 2.5630... ppl: 35.3868  Val Loss: 3.5663\n",
            "Epoch: 8/10... Step: 16480... Loss: 2.6677... ppl: 34.9502  Val Loss: 3.5539\n",
            "Epoch: 8/10... Step: 16512... Loss: 1.7540... ppl: 35.1566  Val Loss: 3.5598\n",
            "Epoch: 8/10... Step: 16544... Loss: 2.6387... ppl: 35.3246  Val Loss: 3.5646\n",
            "Epoch: 8/10... Step: 16576... Loss: 2.8496... ppl: 35.1015  Val Loss: 3.5582\n",
            "Epoch: 8/10... Step: 16608... Loss: 2.8141... ppl: 35.2572  Val Loss: 3.5627\n",
            "Epoch: 8/10... Step: 16640... Loss: 2.3829... ppl: 35.3175  Val Loss: 3.5644\n",
            "Epoch: 8/10... Step: 16672... Loss: 2.8290... ppl: 35.4419  Val Loss: 3.5679\n",
            "Epoch: 8/10... Step: 16704... Loss: 2.8257... ppl: 35.7383  Val Loss: 3.5762\n",
            "Epoch: 8/10... Step: 16736... Loss: 2.9413... ppl: 35.6546  Val Loss: 3.5739\n",
            "Epoch: 8/10... Step: 16768... Loss: 2.8538... ppl: 35.3185  Val Loss: 3.5644\n",
            "Epoch: 8/10... Step: 16800... Loss: 2.2576... ppl: 35.2179  Val Loss: 3.5616\n",
            "Epoch: 8/10... Step: 16832... Loss: 2.3977... ppl: 35.4450  Val Loss: 3.5680\n",
            "Epoch: 8/10... Step: 16864... Loss: 2.5983... ppl: 35.7844  Val Loss: 3.5775\n",
            "Epoch: 8/10... Step: 16896... Loss: 2.4049... ppl: 35.8362  Val Loss: 3.5790\n",
            "Epoch: 8/10... Step: 16928... Loss: 2.4174... ppl: 35.4776  Val Loss: 3.5689\n",
            "Epoch: 8/10... Step: 16960... Loss: 2.4440... ppl: 35.5227  Val Loss: 3.5702\n",
            "Epoch: 8/10... Step: 16992... Loss: 2.6138... ppl: 35.7119  Val Loss: 3.5755\n",
            "Epoch: 8/10... Step: 17024... Loss: 2.2245... ppl: 35.6312  Val Loss: 3.5732\n",
            "Epoch: 8/10... Step: 17056... Loss: 2.6191... ppl: 35.7983  Val Loss: 3.5779\n",
            "Epoch: 8/10... Step: 17088... Loss: 3.2233... ppl: 35.9027  Val Loss: 3.5808\n",
            "Epoch: 8/10... Step: 17120... Loss: 2.5721... ppl: 35.9356  Val Loss: 3.5817\n",
            "Epoch: 8/10... Step: 17152... Loss: 3.0369... ppl: 36.0157  Val Loss: 3.5840\n",
            "Epoch: 8/10... Step: 17184... Loss: 2.6289... ppl: 35.7124  Val Loss: 3.5755\n",
            "Epoch: 8/10... Step: 17216... Loss: 2.4049... ppl: 35.8731  Val Loss: 3.5800\n",
            "Epoch: 8/10... Step: 17248... Loss: 2.3618... ppl: 36.2378  Val Loss: 3.5901\n",
            "Epoch: 8/10... Step: 17280... Loss: 2.3483... ppl: 35.9589  Val Loss: 3.5824\n",
            "Epoch: 8/10... Step: 17312... Loss: 3.5011... ppl: 35.7774  Val Loss: 3.5773\n",
            "Epoch: 8/10... Step: 17344... Loss: 2.4977... ppl: 35.8721  Val Loss: 3.5800\n",
            "Epoch: 8/10... Step: 17376... Loss: 2.8199... ppl: 36.0629  Val Loss: 3.5853\n",
            "Epoch: 8/10... Step: 17408... Loss: 2.6945... ppl: 36.0467  Val Loss: 3.5848\n",
            "Epoch: 8/10... Step: 17440... Loss: 2.6696... ppl: 36.2887  Val Loss: 3.5915\n",
            "Epoch: 8/10... Step: 17472... Loss: 2.7295... ppl: 36.2486  Val Loss: 3.5904\n",
            "Epoch: 8/10... Step: 17504... Loss: 2.7008... ppl: 35.9840  Val Loss: 3.5831\n",
            "Epoch: 8/10... Step: 17536... Loss: 2.7502... ppl: 36.0326  Val Loss: 3.5844\n",
            "Epoch: 8/10... Step: 17568... Loss: 2.8938... ppl: 35.9146  Val Loss: 3.5811\n",
            "Epoch: 8/10... Step: 17600... Loss: 2.7171... ppl: 35.8085  Val Loss: 3.5782\n",
            "Epoch: 8/10... Step: 17632... Loss: 2.6076... ppl: 35.7626  Val Loss: 3.5769\n",
            "Epoch: 8/10... Step: 17664... Loss: 2.5412... ppl: 35.8978  Val Loss: 3.5807\n",
            "Epoch: 8/10... Step: 17696... Loss: 3.1170... ppl: 35.8165  Val Loss: 3.5784\n",
            "Epoch: 8/10... Step: 17728... Loss: 3.1669... ppl: 35.7017  Val Loss: 3.5752\n",
            "Epoch: 8/10... Step: 17760... Loss: 2.5792... ppl: 36.1893  Val Loss: 3.5888\n",
            "Epoch: 8/10... Step: 17792... Loss: 2.5029... ppl: 35.8999  Val Loss: 3.5807\n",
            "Epoch: 8/10... Step: 17824... Loss: 2.7202... ppl: 35.8676  Val Loss: 3.5798\n",
            "Epoch: 8/10... Step: 17856... Loss: 2.4262... ppl: 35.9476  Val Loss: 3.5821\n",
            "Epoch: 8/10... Step: 17888... Loss: 2.7995... ppl: 35.8095  Val Loss: 3.5782\n",
            "Epoch: 8/10... Step: 17920... Loss: 2.1655... ppl: 35.7621  Val Loss: 3.5769\n",
            "Epoch: 8/10... Step: 17952... Loss: 2.7107... ppl: 35.8718  Val Loss: 3.5800\n",
            "Epoch: 8/10... Step: 17984... Loss: 2.6823... ppl: 36.0170  Val Loss: 3.5840\n",
            "Epoch: 8/10... Step: 18016... Loss: 2.7476... ppl: 36.0522  Val Loss: 3.5850\n",
            "Epoch: 8/10... Step: 18048... Loss: 3.1109... ppl: 35.8253  Val Loss: 3.5787\n",
            "Epoch: 8/10... Step: 18080... Loss: 2.9224... ppl: 35.8614  Val Loss: 3.5797\n",
            "Epoch: 8/10... Step: 18112... Loss: 2.3653... ppl: 35.9339  Val Loss: 3.5817\n",
            "Epoch: 8/10... Step: 18144... Loss: 3.3748... ppl: 35.8557  Val Loss: 3.5795\n",
            "Epoch: 8/10... Step: 18176... Loss: 2.8411... ppl: 35.8400  Val Loss: 3.5791\n",
            "Epoch: 8/10... Step: 18208... Loss: 2.7621... ppl: 36.0119  Val Loss: 3.5838\n",
            "Epoch: 8/10... Step: 18240... Loss: 2.8484... ppl: 36.1714  Val Loss: 3.5883\n",
            "Epoch: 8/10... Step: 18272... Loss: 2.0592... ppl: 36.0555  Val Loss: 3.5851\n",
            "Epoch: 8/10... Step: 18304... Loss: 2.7773... ppl: 36.2589  Val Loss: 3.5907\n",
            "Epoch: 8/10... Step: 18336... Loss: 2.9422... ppl: 36.0892  Val Loss: 3.5860\n",
            "Epoch: 8/10... Step: 18368... Loss: 2.8939... ppl: 35.8403  Val Loss: 3.5791\n",
            "Epoch: 8/10... Step: 18400... Loss: 2.7290... ppl: 36.3892  Val Loss: 3.5943\n",
            "Epoch: 8/10... Step: 18432... Loss: 2.9596... ppl: 36.1262  Val Loss: 3.5870\n",
            "Epoch: 8/10... Step: 18464... Loss: 2.3726... ppl: 36.0047  Val Loss: 3.5837\n",
            "Epoch: 8/10... Step: 18496... Loss: 2.0805... ppl: 36.1640  Val Loss: 3.5881\n",
            "Epoch: 8/10... Step: 18528... Loss: 2.9620... ppl: 36.3155  Val Loss: 3.5922\n",
            "Epoch: 8/10... Step: 18560... Loss: 2.4003... ppl: 36.0308  Val Loss: 3.5844\n",
            "Epoch: 8/10... Step: 18592... Loss: 2.7006... ppl: 36.1454  Val Loss: 3.5876\n",
            "Epoch: 8/10... Step: 18624... Loss: 2.6637... ppl: 36.1560  Val Loss: 3.5878\n",
            "Epoch: 8/10... Step: 18656... Loss: 3.5031... ppl: 36.7243  Val Loss: 3.6034\n",
            "Epoch: 8/10... Step: 18688... Loss: 2.1987... ppl: 36.5313  Val Loss: 3.5982\n",
            "Epoch: 8/10... Step: 18720... Loss: 2.5239... ppl: 36.2144  Val Loss: 3.5895\n",
            "Epoch: 9/10... Step: 18752... Loss: 2.4570... ppl: 35.9442  Val Loss: 3.5820\n",
            "Epoch: 9/10... Step: 18784... Loss: 2.5169... ppl: 36.2946  Val Loss: 3.5917\n",
            "Epoch: 9/10... Step: 18816... Loss: 2.8643... ppl: 35.9341  Val Loss: 3.5817\n",
            "Epoch: 9/10... Step: 18848... Loss: 2.8683... ppl: 35.9870  Val Loss: 3.5832\n",
            "Epoch: 9/10... Step: 18880... Loss: 2.8077... ppl: 36.3629  Val Loss: 3.5936\n",
            "Epoch: 9/10... Step: 18912... Loss: 2.8676... ppl: 36.0880  Val Loss: 3.5860\n",
            "Epoch: 9/10... Step: 18944... Loss: 2.7228... ppl: 36.2558  Val Loss: 3.5906\n",
            "Epoch: 9/10... Step: 18976... Loss: 2.7398... ppl: 36.2841  Val Loss: 3.5914\n",
            "Epoch: 9/10... Step: 19008... Loss: 3.0058... ppl: 36.4626  Val Loss: 3.5963\n",
            "Epoch: 9/10... Step: 19040... Loss: 2.4978... ppl: 36.5724  Val Loss: 3.5993\n",
            "Epoch: 9/10... Step: 19072... Loss: 2.4242... ppl: 36.8090  Val Loss: 3.6057\n",
            "Epoch: 9/10... Step: 19104... Loss: 2.4758... ppl: 36.4929  Val Loss: 3.5971\n",
            "Epoch: 9/10... Step: 19136... Loss: 1.9836... ppl: 36.2363  Val Loss: 3.5901\n",
            "Epoch: 9/10... Step: 19168... Loss: 2.3031... ppl: 36.3441  Val Loss: 3.5930\n",
            "Epoch: 9/10... Step: 19200... Loss: 3.0379... ppl: 36.5405  Val Loss: 3.5984\n",
            "Epoch: 9/10... Step: 19232... Loss: 2.7739... ppl: 36.9308  Val Loss: 3.6090\n",
            "Epoch: 9/10... Step: 19264... Loss: 2.7864... ppl: 36.5267  Val Loss: 3.5980\n",
            "Epoch: 9/10... Step: 19296... Loss: 3.1801... ppl: 36.6553  Val Loss: 3.6016\n",
            "Epoch: 9/10... Step: 19328... Loss: 2.8005... ppl: 36.6778  Val Loss: 3.6022\n",
            "Epoch: 9/10... Step: 19360... Loss: 2.9447... ppl: 36.7039  Val Loss: 3.6029\n",
            "Epoch: 9/10... Step: 19392... Loss: 2.4681... ppl: 36.8561  Val Loss: 3.6070\n",
            "Epoch: 9/10... Step: 19424... Loss: 2.7661... ppl: 36.9264  Val Loss: 3.6089\n",
            "Epoch: 9/10... Step: 19456... Loss: 2.6011... ppl: 36.9498  Val Loss: 3.6096\n",
            "Epoch: 9/10... Step: 19488... Loss: 2.5003... ppl: 37.1174  Val Loss: 3.6141\n",
            "Epoch: 9/10... Step: 19520... Loss: 2.6724... ppl: 36.8828  Val Loss: 3.6077\n",
            "Epoch: 9/10... Step: 19552... Loss: 2.1923... ppl: 36.8240  Val Loss: 3.6061\n",
            "Epoch: 9/10... Step: 19584... Loss: 2.1545... ppl: 37.1427  Val Loss: 3.6148\n",
            "Epoch: 9/10... Step: 19616... Loss: 2.7227... ppl: 37.1670  Val Loss: 3.6154\n",
            "Epoch: 9/10... Step: 19648... Loss: 2.8084... ppl: 36.7494  Val Loss: 3.6041\n",
            "Epoch: 9/10... Step: 19680... Loss: 2.7193... ppl: 36.7523  Val Loss: 3.6042\n",
            "Epoch: 9/10... Step: 19712... Loss: 2.2467... ppl: 37.0943  Val Loss: 3.6135\n",
            "Epoch: 9/10... Step: 19744... Loss: 1.9713... ppl: 37.1874  Val Loss: 3.6160\n",
            "Epoch: 9/10... Step: 19776... Loss: 3.0083... ppl: 37.3040  Val Loss: 3.6191\n",
            "Epoch: 9/10... Step: 19808... Loss: 2.0735... ppl: 37.3368  Val Loss: 3.6200\n",
            "Epoch: 9/10... Step: 19840... Loss: 2.3108... ppl: 37.0893  Val Loss: 3.6133\n",
            "Epoch: 9/10... Step: 19872... Loss: 2.7510... ppl: 37.0553  Val Loss: 3.6124\n",
            "Epoch: 9/10... Step: 19904... Loss: 2.4976... ppl: 37.0493  Val Loss: 3.6122\n",
            "Epoch: 9/10... Step: 19936... Loss: 2.4031... ppl: 36.8508  Val Loss: 3.6069\n",
            "Epoch: 9/10... Step: 19968... Loss: 2.4764... ppl: 36.7866  Val Loss: 3.6051\n",
            "Epoch: 9/10... Step: 20000... Loss: 3.0517... ppl: 36.9069  Val Loss: 3.6084\n",
            "Epoch: 9/10... Step: 20032... Loss: 2.8539... ppl: 36.8419  Val Loss: 3.6066\n",
            "Epoch: 9/10... Step: 20064... Loss: 2.4914... ppl: 36.6788  Val Loss: 3.6022\n",
            "Epoch: 9/10... Step: 20096... Loss: 2.6399... ppl: 37.0822  Val Loss: 3.6131\n",
            "Epoch: 9/10... Step: 20128... Loss: 2.6953... ppl: 37.1535  Val Loss: 3.6151\n",
            "Epoch: 9/10... Step: 20160... Loss: 2.9297... ppl: 36.9054  Val Loss: 3.6084\n",
            "Epoch: 9/10... Step: 20192... Loss: 2.2745... ppl: 36.9517  Val Loss: 3.6096\n",
            "Epoch: 9/10... Step: 20224... Loss: 2.3721... ppl: 36.8459  Val Loss: 3.6067\n",
            "Epoch: 9/10... Step: 20256... Loss: 2.3114... ppl: 36.6721  Val Loss: 3.6020\n",
            "Epoch: 9/10... Step: 20288... Loss: 2.7688... ppl: 36.7995  Val Loss: 3.6055\n",
            "Epoch: 9/10... Step: 20320... Loss: 2.5791... ppl: 37.0732  Val Loss: 3.6129\n",
            "Epoch: 9/10... Step: 20352... Loss: 2.3371... ppl: 37.0794  Val Loss: 3.6131\n",
            "Epoch: 9/10... Step: 20384... Loss: 2.7194... ppl: 37.0011  Val Loss: 3.6109\n",
            "Epoch: 9/10... Step: 20416... Loss: 2.7111... ppl: 36.8255  Val Loss: 3.6062\n",
            "Epoch: 9/10... Step: 20448... Loss: 2.4278... ppl: 37.0022  Val Loss: 3.6110\n",
            "Epoch: 9/10... Step: 20480... Loss: 2.5035... ppl: 36.9636  Val Loss: 3.6099\n",
            "Epoch: 9/10... Step: 20512... Loss: 2.7991... ppl: 36.8797  Val Loss: 3.6077\n",
            "Epoch: 9/10... Step: 20544... Loss: 2.5317... ppl: 37.0642  Val Loss: 3.6127\n",
            "Epoch: 9/10... Step: 20576... Loss: 2.5441... ppl: 37.3329  Val Loss: 3.6199\n",
            "Epoch: 9/10... Step: 20608... Loss: 2.4686... ppl: 37.0884  Val Loss: 3.6133\n",
            "Epoch: 9/10... Step: 20640... Loss: 2.3327... ppl: 37.2969  Val Loss: 3.6189\n",
            "Epoch: 9/10... Step: 20672... Loss: 2.6442... ppl: 37.2998  Val Loss: 3.6190\n",
            "Epoch: 9/10... Step: 20704... Loss: 2.6897... ppl: 36.8502  Val Loss: 3.6069\n",
            "Epoch: 9/10... Step: 20736... Loss: 2.4269... ppl: 37.3447  Val Loss: 3.6202\n",
            "Epoch: 9/10... Step: 20768... Loss: 2.9162... ppl: 37.1857  Val Loss: 3.6159\n",
            "Epoch: 9/10... Step: 20800... Loss: 2.7836... ppl: 37.1319  Val Loss: 3.6145\n",
            "Epoch: 9/10... Step: 20832... Loss: 2.8152... ppl: 37.2031  Val Loss: 3.6164\n",
            "Epoch: 9/10... Step: 20864... Loss: 2.3954... ppl: 37.3744  Val Loss: 3.6210\n",
            "Epoch: 9/10... Step: 20896... Loss: 2.6506... ppl: 37.1484  Val Loss: 3.6149\n",
            "Epoch: 9/10... Step: 20928... Loss: 2.4395... ppl: 37.1321  Val Loss: 3.6145\n",
            "Epoch: 9/10... Step: 20960... Loss: 2.5313... ppl: 37.1637  Val Loss: 3.6153\n",
            "Epoch: 9/10... Step: 20992... Loss: 2.1582... ppl: 37.7554  Val Loss: 3.6311\n",
            "Epoch: 9/10... Step: 21024... Loss: 2.4025... ppl: 37.6093  Val Loss: 3.6273\n",
            "Epoch: 9/10... Step: 21056... Loss: 2.5655... ppl: 37.3980  Val Loss: 3.6216\n",
            "Epoch: 10/10... Step: 21088... Loss: 2.6923... ppl: 37.0628  Val Loss: 3.6126\n",
            "Epoch: 10/10... Step: 21120... Loss: 2.4439... ppl: 37.3350  Val Loss: 3.6199\n",
            "Epoch: 10/10... Step: 21152... Loss: 2.6313... ppl: 37.1390  Val Loss: 3.6147\n",
            "Epoch: 10/10... Step: 21184... Loss: 2.6690... ppl: 36.9868  Val Loss: 3.6106\n",
            "Epoch: 10/10... Step: 21216... Loss: 2.2514... ppl: 37.2208  Val Loss: 3.6169\n",
            "Epoch: 10/10... Step: 21248... Loss: 2.3793... ppl: 37.1478  Val Loss: 3.6149\n",
            "Epoch: 10/10... Step: 21280... Loss: 3.1574... ppl: 37.3258  Val Loss: 3.6197\n",
            "Epoch: 10/10... Step: 21312... Loss: 2.6881... ppl: 37.3559  Val Loss: 3.6205\n",
            "Epoch: 10/10... Step: 21344... Loss: 2.6500... ppl: 37.4962  Val Loss: 3.6242\n",
            "Epoch: 10/10... Step: 21376... Loss: 2.4051... ppl: 37.3998  Val Loss: 3.6217\n",
            "Epoch: 10/10... Step: 21408... Loss: 2.7018... ppl: 37.7267  Val Loss: 3.6304\n",
            "Epoch: 10/10... Step: 21440... Loss: 2.7186... ppl: 37.4790  Val Loss: 3.6238\n",
            "Epoch: 10/10... Step: 21472... Loss: 2.5044... ppl: 37.2067  Val Loss: 3.6165\n",
            "Epoch: 10/10... Step: 21504... Loss: 2.6694... ppl: 37.2523  Val Loss: 3.6177\n",
            "Epoch: 10/10... Step: 21536... Loss: 2.4884... ppl: 37.5271  Val Loss: 3.6251\n",
            "Epoch: 10/10... Step: 21568... Loss: 2.6745... ppl: 37.9167  Val Loss: 3.6354\n",
            "Epoch: 10/10... Step: 21600... Loss: 2.3561... ppl: 37.7930  Val Loss: 3.6321\n",
            "Epoch: 10/10... Step: 21632... Loss: 2.5206... ppl: 37.8282  Val Loss: 3.6331\n",
            "Epoch: 10/10... Step: 21664... Loss: 2.4532... ppl: 37.7550  Val Loss: 3.6311\n",
            "Epoch: 10/10... Step: 21696... Loss: 2.8721... ppl: 37.8929  Val Loss: 3.6348\n",
            "Epoch: 10/10... Step: 21728... Loss: 2.4494... ppl: 37.9220  Val Loss: 3.6355\n",
            "Epoch: 10/10... Step: 21760... Loss: 2.0888... ppl: 38.0605  Val Loss: 3.6392\n",
            "Epoch: 10/10... Step: 21792... Loss: 2.2240... ppl: 38.2190  Val Loss: 3.6433\n",
            "Epoch: 10/10... Step: 21824... Loss: 2.4770... ppl: 38.2088  Val Loss: 3.6431\n",
            "Epoch: 10/10... Step: 21856... Loss: 2.6363... ppl: 38.1673  Val Loss: 3.6420\n",
            "Epoch: 10/10... Step: 21888... Loss: 2.3358... ppl: 38.0425  Val Loss: 3.6387\n",
            "Epoch: 10/10... Step: 21920... Loss: 2.9049... ppl: 38.2176  Val Loss: 3.6433\n",
            "Epoch: 10/10... Step: 21952... Loss: 2.7650... ppl: 38.5207  Val Loss: 3.6512\n",
            "Epoch: 10/10... Step: 21984... Loss: 1.9737... ppl: 37.8925  Val Loss: 3.6348\n",
            "Epoch: 10/10... Step: 22016... Loss: 2.6048... ppl: 37.7862  Val Loss: 3.6319\n",
            "Epoch: 10/10... Step: 22048... Loss: 2.1881... ppl: 38.0923  Val Loss: 3.6400\n",
            "Epoch: 10/10... Step: 22080... Loss: 2.8122... ppl: 38.2943  Val Loss: 3.6453\n",
            "Epoch: 10/10... Step: 22112... Loss: 2.1439... ppl: 38.2474  Val Loss: 3.6441\n",
            "Epoch: 10/10... Step: 22144... Loss: 2.2866... ppl: 38.4793  Val Loss: 3.6501\n",
            "Epoch: 10/10... Step: 22176... Loss: 2.2257... ppl: 38.1356  Val Loss: 3.6411\n",
            "Epoch: 10/10... Step: 22208... Loss: 2.9298... ppl: 38.1940  Val Loss: 3.6427\n",
            "Epoch: 10/10... Step: 22240... Loss: 2.4887... ppl: 38.2977  Val Loss: 3.6454\n",
            "Epoch: 10/10... Step: 22272... Loss: 2.7941... ppl: 37.9336  Val Loss: 3.6358\n",
            "Epoch: 10/10... Step: 22304... Loss: 2.7370... ppl: 37.8797  Val Loss: 3.6344\n",
            "Epoch: 10/10... Step: 22336... Loss: 2.4502... ppl: 38.0421  Val Loss: 3.6387\n",
            "Epoch: 10/10... Step: 22368... Loss: 3.0017... ppl: 37.9592  Val Loss: 3.6365\n",
            "Epoch: 10/10... Step: 22400... Loss: 2.4964... ppl: 37.8431  Val Loss: 3.6334\n",
            "Epoch: 10/10... Step: 22432... Loss: 2.3478... ppl: 38.0709  Val Loss: 3.6395\n",
            "Epoch: 10/10... Step: 22464... Loss: 1.8079... ppl: 38.4102  Val Loss: 3.6483\n",
            "Epoch: 10/10... Step: 22496... Loss: 2.3748... ppl: 38.0135  Val Loss: 3.6379\n",
            "Epoch: 10/10... Step: 22528... Loss: 2.8920... ppl: 38.1614  Val Loss: 3.6418\n",
            "Epoch: 10/10... Step: 22560... Loss: 2.6039... ppl: 37.9829  Val Loss: 3.6371\n",
            "Epoch: 10/10... Step: 22592... Loss: 2.4160... ppl: 37.8640  Val Loss: 3.6340\n",
            "Epoch: 10/10... Step: 22624... Loss: 2.9469... ppl: 37.9624  Val Loss: 3.6366\n",
            "Epoch: 10/10... Step: 22656... Loss: 2.3773... ppl: 38.1545  Val Loss: 3.6416\n",
            "Epoch: 10/10... Step: 22688... Loss: 2.8588... ppl: 38.2349  Val Loss: 3.6437\n",
            "Epoch: 10/10... Step: 22720... Loss: 2.3778... ppl: 38.1908  Val Loss: 3.6426\n",
            "Epoch: 10/10... Step: 22752... Loss: 2.9131... ppl: 38.0857  Val Loss: 3.6398\n",
            "Epoch: 10/10... Step: 22784... Loss: 2.6981... ppl: 38.3642  Val Loss: 3.6471\n",
            "Epoch: 10/10... Step: 22816... Loss: 2.5320... ppl: 38.4185  Val Loss: 3.6485\n",
            "Epoch: 10/10... Step: 22848... Loss: 2.8667... ppl: 38.1205  Val Loss: 3.6408\n",
            "Epoch: 10/10... Step: 22880... Loss: 2.0996... ppl: 38.3121  Val Loss: 3.6458\n",
            "Epoch: 10/10... Step: 22912... Loss: 1.8941... ppl: 38.4319  Val Loss: 3.6489\n",
            "Epoch: 10/10... Step: 22944... Loss: 2.8539... ppl: 38.2993  Val Loss: 3.6454\n",
            "Epoch: 10/10... Step: 22976... Loss: 2.4726... ppl: 38.3630  Val Loss: 3.6471\n",
            "Epoch: 10/10... Step: 23008... Loss: 2.6478... ppl: 38.6695  Val Loss: 3.6551\n",
            "Epoch: 10/10... Step: 23040... Loss: 2.2439... ppl: 38.2348  Val Loss: 3.6437\n",
            "Epoch: 10/10... Step: 23072... Loss: 2.5489... ppl: 38.4399  Val Loss: 3.6491\n",
            "Epoch: 10/10... Step: 23104... Loss: 2.5178... ppl: 38.5333  Val Loss: 3.6515\n",
            "Epoch: 10/10... Step: 23136... Loss: 2.4794... ppl: 38.3740  Val Loss: 3.6474\n",
            "Epoch: 10/10... Step: 23168... Loss: 2.6794... ppl: 38.4820  Val Loss: 3.6502\n",
            "Epoch: 10/10... Step: 23200... Loss: 2.2681... ppl: 38.6687  Val Loss: 3.6550\n",
            "Epoch: 10/10... Step: 23232... Loss: 2.4783... ppl: 38.6224  Val Loss: 3.6538\n",
            "Epoch: 10/10... Step: 23264... Loss: 2.4957... ppl: 38.3136  Val Loss: 3.6458\n",
            "Epoch: 10/10... Step: 23296... Loss: 2.5634... ppl: 38.4376  Val Loss: 3.6490\n",
            "Epoch: 10/10... Step: 23328... Loss: 2.5128... ppl: 38.9505  Val Loss: 3.6623\n",
            "Epoch: 10/10... Step: 23360... Loss: 3.1148... ppl: 38.9592  Val Loss: 3.6625\n",
            "Epoch: 10/10... Step: 23392... Loss: 2.6656... ppl: 38.8118  Val Loss: 3.6587\n",
            "Epoch: 10/10... Step: 23424... Loss: 2.0239... ppl: 38.1861  Val Loss: 3.6425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ6leiAm0Wmn"
      },
      "source": [
        "# 6. Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPoIBPQpgqZy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "692837a5-87de-4b44-8659-afd606e85b2e"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "net.load_state_dict(torch.load(path))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'net' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-940e25f5c056>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load weights of best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'saved_weights.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bch2a0WglQ96"
      },
      "source": [
        "# function to generate one token\n",
        "def predict(net, tkn, h=None):\n",
        "\n",
        "  # tensor inputs\n",
        "  x = np.array([[token2int[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  if(torch.cuda.is_available()):\n",
        "      inputs = inputs.cuda()\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  if(torch.cuda.is_available()):\n",
        "      p = p.cpu()\n",
        "\n",
        "  p = p.numpy()\n",
        "  sampled_token_index = np.argmax(p, axis = 1)[0]\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int2token[sampled_token_index], h"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQR-euTiFan9"
      },
      "source": [
        "# function to fetch generated sequence\n",
        "def sample(net, size = 2, seed_text='it is'):\n",
        "\n",
        "    if(torch.cuda.is_available()):\n",
        "        net.cuda()\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    toks = seed_text.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in toks:\n",
        "      token, h = predict(net, t, h)\n",
        "\n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(net, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSakRw3SHRv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad751fa-64ae-46ce-976c-82a7c5d10d42"
      },
      "source": [
        "# seed texts\n",
        "seeds = [\"i want to\",\n",
        "         \"how about a cup\",\n",
        "         \"i don't want\",\n",
        "         \"can you send\",\n",
        "         \"my car\"]\n",
        "\n",
        "# number of tokens to generate\n",
        "num_toks = 6\n",
        "\n",
        "# text generation\n",
        "for s in seeds:\n",
        "  # get generated text from the model\n",
        "  text_gen = sample(net, num_toks, seed_text=s)\n",
        "  # print the result\n",
        "  print(\"seed text:\", s, \">> output:\",text_gen)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed text: i want to >> output: i want to order a pizza from <unk> pizza\n",
            "\n",
            "\n",
            "seed text: how about a cup >> output: how about a cup of caramel in the one on\n",
            "\n",
            "\n",
            "seed text: i don't want >> output: i don't want to be <unk> and i want\n",
            "\n",
            "\n",
            "seed text: can you send >> output: can you send me the receipt to my phone\n",
            "\n",
            "\n",
            "seed text: my car >> output: my car is making a weird noise when\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x61pT-Zodg3"
      },
      "source": [],
      "execution_count": 71,
      "outputs": []
    }
  ]
}